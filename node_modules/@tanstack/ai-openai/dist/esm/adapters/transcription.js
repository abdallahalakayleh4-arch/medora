import { BaseTranscriptionAdapter } from "@tanstack/ai/adapters";
import { createOpenAIClient, generateId, getOpenAIApiKeyFromEnv } from "../utils/client.js";
class OpenAITranscriptionAdapter extends BaseTranscriptionAdapter {
  constructor(config, model) {
    super(config, model);
    this.name = "openai";
    this.client = createOpenAIClient(config);
  }
  async transcribe(options) {
    const { model, audio, language, prompt, responseFormat, modelOptions } = options;
    const file = this.prepareAudioFile(audio);
    const request = {
      model,
      file,
      language,
      prompt,
      response_format: this.mapResponseFormat(responseFormat),
      ...modelOptions
    };
    const useVerbose = responseFormat === "verbose_json" || !responseFormat && model !== "whisper-1";
    if (useVerbose) {
      const response = await this.client.audio.transcriptions.create({
        ...request,
        response_format: "verbose_json"
      });
      return {
        id: generateId(this.name),
        model,
        text: response.text,
        language: response.language,
        duration: response.duration,
        segments: response.segments?.map(
          (seg) => ({
            id: seg.id,
            start: seg.start,
            end: seg.end,
            text: seg.text,
            confidence: seg.avg_logprob ? Math.exp(seg.avg_logprob) : void 0
          })
        ),
        words: response.words?.map((w) => ({
          word: w.word,
          start: w.start,
          end: w.end
        }))
      };
    } else {
      const response = await this.client.audio.transcriptions.create(request);
      return {
        id: generateId(this.name),
        model,
        text: typeof response === "string" ? response : response.text,
        language
      };
    }
  }
  prepareAudioFile(audio) {
    if (typeof File !== "undefined" && audio instanceof File) {
      return audio;
    }
    if (typeof Blob !== "undefined" && audio instanceof Blob) {
      return new File([audio], "audio.mp3", {
        type: audio.type || "audio/mpeg"
      });
    }
    if (audio instanceof ArrayBuffer) {
      return new File([audio], "audio.mp3", { type: "audio/mpeg" });
    }
    if (typeof audio === "string") {
      if (audio.startsWith("data:")) {
        const parts = audio.split(",");
        const header = parts[0];
        const base64Data = parts[1] || "";
        const mimeMatch = header?.match(/data:([^;]+)/);
        const mimeType = mimeMatch?.[1] || "audio/mpeg";
        const binaryStr2 = atob(base64Data);
        const bytes2 = new Uint8Array(binaryStr2.length);
        for (let i = 0; i < binaryStr2.length; i++) {
          bytes2[i] = binaryStr2.charCodeAt(i);
        }
        const extension = mimeType.split("/")[1] || "mp3";
        return new File([bytes2], `audio.${extension}`, { type: mimeType });
      }
      const binaryStr = atob(audio);
      const bytes = new Uint8Array(binaryStr.length);
      for (let i = 0; i < binaryStr.length; i++) {
        bytes[i] = binaryStr.charCodeAt(i);
      }
      return new File([bytes], "audio.mp3", { type: "audio/mpeg" });
    }
    throw new Error("Invalid audio input type");
  }
  mapResponseFormat(format) {
    if (!format) return "json";
    return format;
  }
}
function createOpenaiTranscription(model, apiKey, config) {
  return new OpenAITranscriptionAdapter({ apiKey, ...config }, model);
}
function openaiTranscription(model, config) {
  const apiKey = getOpenAIApiKeyFromEnv();
  return createOpenaiTranscription(model, apiKey, config);
}
export {
  OpenAITranscriptionAdapter,
  createOpenaiTranscription,
  openaiTranscription
};
//# sourceMappingURL=transcription.js.map
