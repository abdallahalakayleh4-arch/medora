import { BaseTTSAdapter } from "@tanstack/ai/adapters";
import { createOpenAIClient, generateId, getOpenAIApiKeyFromEnv } from "../utils/client.js";
import { validateAudioInput, validateSpeed, validateInstructions } from "../audio/audio-provider-options.js";
class OpenAITTSAdapter extends BaseTTSAdapter {
  constructor(config, model) {
    super(config, model);
    this.name = "openai";
    this.client = createOpenAIClient(config);
  }
  async generateSpeech(options) {
    const { model, text, voice, format, speed, modelOptions } = options;
    const audioOptions = {
      input: text,
      model,
      speed,
      ...modelOptions
    };
    validateAudioInput(audioOptions);
    validateSpeed(audioOptions);
    validateInstructions(audioOptions);
    const request = {
      model,
      input: text,
      voice: voice || "alloy",
      response_format: format,
      speed,
      ...modelOptions
    };
    const response = await this.client.audio.speech.create(request);
    const arrayBuffer = await response.arrayBuffer();
    const base64 = Buffer.from(arrayBuffer).toString("base64");
    const outputFormat = format || "mp3";
    const contentType = this.getContentType(outputFormat);
    return {
      id: generateId(this.name),
      model,
      audio: base64,
      format: outputFormat,
      contentType
    };
  }
  getContentType(format) {
    const contentTypes = {
      mp3: "audio/mpeg",
      opus: "audio/opus",
      aac: "audio/aac",
      flac: "audio/flac",
      wav: "audio/wav",
      pcm: "audio/pcm"
    };
    return contentTypes[format] || "audio/mpeg";
  }
}
function createOpenaiSpeech(model, apiKey, config) {
  return new OpenAITTSAdapter({ apiKey, ...config }, model);
}
function openaiSpeech(model, config) {
  const apiKey = getOpenAIApiKeyFromEnv();
  return createOpenaiSpeech(model, apiKey, config);
}
export {
  OpenAITTSAdapter,
  createOpenaiSpeech,
  openaiSpeech
};
//# sourceMappingURL=tts.js.map
