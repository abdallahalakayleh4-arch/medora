import { BaseTextAdapter } from "@tanstack/ai/adapters";
import { validateTextProviderOptions } from "../text/text-provider-options.js";
import { makeOpenAIStructuredOutputCompatible, transformNullsToUndefined } from "../utils/schema-converter.js";
import { convertToolsToProviderFormat } from "../tools/tool-converter.js";
import { createOpenAIClient, generateId, getOpenAIApiKeyFromEnv } from "../utils/client.js";
class OpenAITextAdapter extends BaseTextAdapter {
  constructor(config, model) {
    super({}, model);
    this.kind = "text";
    this.name = "openai";
    this.client = createOpenAIClient(config);
  }
  async *chatStream(options) {
    const toolCallMetadata = /* @__PURE__ */ new Map();
    const requestArguments = this.mapTextOptionsToOpenAI(options);
    try {
      const response = await this.client.responses.create(
        {
          ...requestArguments,
          stream: true
        },
        {
          headers: options.request?.headers,
          signal: options.request?.signal
        }
      );
      yield* this.processOpenAIStreamChunks(
        response,
        toolCallMetadata,
        options,
        () => generateId(this.name)
      );
    } catch (error) {
      const err = error;
      console.error(">>> chatStream: Fatal error during response creation <<<");
      console.error(">>> Error message:", err.message);
      console.error(">>> Error stack:", err.stack);
      console.error(">>> Full error:", err);
      throw error;
    }
  }
  /**
   * Generate structured output using OpenAI's native JSON Schema response format.
   * Uses stream: false to get the complete response in one call.
   *
   * OpenAI has strict requirements for structured output:
   * - All properties must be in the `required` array
   * - Optional fields should have null added to their type union
   * - additionalProperties must be false for all objects
   *
   * The outputSchema is already JSON Schema (converted in the ai layer).
   * We apply OpenAI-specific transformations for structured output compatibility.
   */
  async structuredOutput(options) {
    const { chatOptions, outputSchema } = options;
    const requestArguments = this.mapTextOptionsToOpenAI(chatOptions);
    const jsonSchema = makeOpenAIStructuredOutputCompatible(
      outputSchema,
      outputSchema.required || []
    );
    try {
      const response = await this.client.responses.create(
        {
          ...requestArguments,
          stream: false,
          // Configure structured output via text.format
          text: {
            format: {
              type: "json_schema",
              name: "structured_output",
              schema: jsonSchema,
              strict: true
            }
          }
        },
        {
          headers: chatOptions.request?.headers,
          signal: chatOptions.request?.signal
        }
      );
      const rawText = this.extractTextFromResponse(response);
      let parsed;
      try {
        parsed = JSON.parse(rawText);
      } catch {
        throw new Error(
          `Failed to parse structured output as JSON. Content: ${rawText.slice(0, 200)}${rawText.length > 200 ? "..." : ""}`
        );
      }
      const transformed = transformNullsToUndefined(parsed);
      return {
        data: transformed,
        rawText
      };
    } catch (error) {
      const err = error;
      console.error(">>> structuredOutput: Error during response creation <<<");
      console.error(">>> Error message:", err.message);
      throw error;
    }
  }
  /**
   * Extract text content from a non-streaming response
   */
  extractTextFromResponse(response) {
    let textContent = "";
    for (const item of response.output) {
      if (item.type === "message") {
        for (const part of item.content) {
          if (part.type === "output_text") {
            textContent += part.text;
          }
        }
      }
    }
    return textContent;
  }
  async *processOpenAIStreamChunks(stream, toolCallMetadata, options, genId) {
    let accumulatedContent = "";
    let accumulatedReasoning = "";
    const timestamp = Date.now();
    let chunkCount = 0;
    let hasStreamedContentDeltas = false;
    let hasStreamedReasoningDeltas = false;
    let model = options.model;
    const runId = genId();
    const messageId = genId();
    let stepId = null;
    let hasEmittedRunStarted = false;
    let hasEmittedTextMessageStart = false;
    let hasEmittedStepStarted = false;
    try {
      for await (const chunk of stream) {
        chunkCount++;
        if (!hasEmittedRunStarted) {
          hasEmittedRunStarted = true;
          yield {
            type: "RUN_STARTED",
            runId,
            model: model || options.model,
            timestamp
          };
        }
        const handleContentPart = (contentPart) => {
          if (contentPart.type === "output_text") {
            accumulatedContent += contentPart.text;
            return {
              type: "TEXT_MESSAGE_CONTENT",
              messageId,
              model: model || options.model,
              timestamp,
              delta: contentPart.text,
              content: accumulatedContent
            };
          }
          if (contentPart.type === "reasoning_text") {
            accumulatedReasoning += contentPart.text;
            return {
              type: "STEP_FINISHED",
              stepId: stepId || genId(),
              model: model || options.model,
              timestamp,
              delta: contentPart.text,
              content: accumulatedReasoning
            };
          }
          return {
            type: "RUN_ERROR",
            runId,
            model: model || options.model,
            timestamp,
            error: {
              message: contentPart.refusal
            }
          };
        };
        if (chunk.type === "response.created" || chunk.type === "response.incomplete" || chunk.type === "response.failed") {
          model = chunk.response.model;
          hasStreamedContentDeltas = false;
          hasStreamedReasoningDeltas = false;
          hasEmittedTextMessageStart = false;
          hasEmittedStepStarted = false;
          accumulatedContent = "";
          accumulatedReasoning = "";
          if (chunk.response.error) {
            yield {
              type: "RUN_ERROR",
              runId,
              model: chunk.response.model,
              timestamp,
              error: chunk.response.error
            };
          }
          if (chunk.response.incomplete_details) {
            yield {
              type: "RUN_ERROR",
              runId,
              model: chunk.response.model,
              timestamp,
              error: {
                message: chunk.response.incomplete_details.reason ?? ""
              }
            };
          }
        }
        if (chunk.type === "response.output_text.delta" && chunk.delta) {
          const textDelta = Array.isArray(chunk.delta) ? chunk.delta.join("") : typeof chunk.delta === "string" ? chunk.delta : "";
          if (textDelta) {
            if (!hasEmittedTextMessageStart) {
              hasEmittedTextMessageStart = true;
              yield {
                type: "TEXT_MESSAGE_START",
                messageId,
                model: model || options.model,
                timestamp,
                role: "assistant"
              };
            }
            accumulatedContent += textDelta;
            hasStreamedContentDeltas = true;
            yield {
              type: "TEXT_MESSAGE_CONTENT",
              messageId,
              model: model || options.model,
              timestamp,
              delta: textDelta,
              content: accumulatedContent
            };
          }
        }
        if (chunk.type === "response.reasoning_text.delta" && chunk.delta) {
          const reasoningDelta = Array.isArray(chunk.delta) ? chunk.delta.join("") : typeof chunk.delta === "string" ? chunk.delta : "";
          if (reasoningDelta) {
            if (!hasEmittedStepStarted) {
              hasEmittedStepStarted = true;
              stepId = genId();
              yield {
                type: "STEP_STARTED",
                stepId,
                model: model || options.model,
                timestamp,
                stepType: "thinking"
              };
            }
            accumulatedReasoning += reasoningDelta;
            hasStreamedReasoningDeltas = true;
            yield {
              type: "STEP_FINISHED",
              stepId: stepId || genId(),
              model: model || options.model,
              timestamp,
              delta: reasoningDelta,
              content: accumulatedReasoning
            };
          }
        }
        if (chunk.type === "response.reasoning_summary_text.delta" && chunk.delta) {
          const summaryDelta = typeof chunk.delta === "string" ? chunk.delta : "";
          if (summaryDelta) {
            if (!hasEmittedStepStarted) {
              hasEmittedStepStarted = true;
              stepId = genId();
              yield {
                type: "STEP_STARTED",
                stepId,
                model: model || options.model,
                timestamp,
                stepType: "thinking"
              };
            }
            accumulatedReasoning += summaryDelta;
            hasStreamedReasoningDeltas = true;
            yield {
              type: "STEP_FINISHED",
              stepId: stepId || genId(),
              model: model || options.model,
              timestamp,
              delta: summaryDelta,
              content: accumulatedReasoning
            };
          }
        }
        if (chunk.type === "response.content_part.added") {
          const contentPart = chunk.part;
          if (contentPart.type === "output_text" && !hasEmittedTextMessageStart) {
            hasEmittedTextMessageStart = true;
            yield {
              type: "TEXT_MESSAGE_START",
              messageId,
              model: model || options.model,
              timestamp,
              role: "assistant"
            };
          }
          if (contentPart.type === "reasoning_text" && !hasEmittedStepStarted) {
            hasEmittedStepStarted = true;
            stepId = genId();
            yield {
              type: "STEP_STARTED",
              stepId,
              model: model || options.model,
              timestamp,
              stepType: "thinking"
            };
          }
          yield handleContentPart(contentPart);
        }
        if (chunk.type === "response.content_part.done") {
          const contentPart = chunk.part;
          if (contentPart.type === "output_text" && hasStreamedContentDeltas) {
            continue;
          }
          if (contentPart.type === "reasoning_text" && hasStreamedReasoningDeltas) {
            continue;
          }
          yield handleContentPart(contentPart);
        }
        if (chunk.type === "response.output_item.added") {
          const item = chunk.item;
          if (item.type === "function_call" && item.id) {
            if (!toolCallMetadata.has(item.id)) {
              toolCallMetadata.set(item.id, {
                index: chunk.output_index,
                name: item.name || "",
                started: false
              });
            }
            yield {
              type: "TOOL_CALL_START",
              toolCallId: item.id,
              toolName: item.name || "",
              model: model || options.model,
              timestamp,
              index: chunk.output_index
            };
            toolCallMetadata.get(item.id).started = true;
          }
        }
        if (chunk.type === "response.function_call_arguments.delta" && chunk.delta) {
          const metadata = toolCallMetadata.get(chunk.item_id);
          yield {
            type: "TOOL_CALL_ARGS",
            toolCallId: chunk.item_id,
            model: model || options.model,
            timestamp,
            delta: chunk.delta,
            args: metadata ? void 0 : chunk.delta
            // We don't accumulate here, let caller handle it
          };
        }
        if (chunk.type === "response.function_call_arguments.done") {
          const { item_id } = chunk;
          const metadata = toolCallMetadata.get(item_id);
          const name = metadata?.name || "";
          let parsedInput = {};
          try {
            parsedInput = chunk.arguments ? JSON.parse(chunk.arguments) : {};
          } catch {
            parsedInput = {};
          }
          yield {
            type: "TOOL_CALL_END",
            toolCallId: item_id,
            toolName: name,
            model: model || options.model,
            timestamp,
            input: parsedInput
          };
        }
        if (chunk.type === "response.completed") {
          if (hasEmittedTextMessageStart) {
            yield {
              type: "TEXT_MESSAGE_END",
              messageId,
              model: model || options.model,
              timestamp
            };
          }
          const hasFunctionCalls = chunk.response.output.some(
            (item) => item.type === "function_call"
          );
          yield {
            type: "RUN_FINISHED",
            runId,
            model: model || options.model,
            timestamp,
            usage: {
              promptTokens: chunk.response.usage?.input_tokens || 0,
              completionTokens: chunk.response.usage?.output_tokens || 0,
              totalTokens: chunk.response.usage?.total_tokens || 0
            },
            finishReason: hasFunctionCalls ? "tool_calls" : "stop"
          };
        }
        if (chunk.type === "error") {
          yield {
            type: "RUN_ERROR",
            runId,
            model: model || options.model,
            timestamp,
            error: {
              message: chunk.message,
              code: chunk.code ?? void 0
            }
          };
        }
      }
    } catch (error) {
      const err = error;
      console.log(
        "[OpenAI Adapter] Stream ended with error. Event type summary:",
        {
          totalChunks: chunkCount,
          error: err.message
        }
      );
      yield {
        type: "RUN_ERROR",
        runId,
        model: options.model,
        timestamp,
        error: {
          message: err.message || "Unknown error occurred",
          code: err.code
        }
      };
    }
  }
  /**
   * Maps common options to OpenAI-specific format
   * Handles translation of normalized options to OpenAI's API format
   */
  mapTextOptionsToOpenAI(options) {
    const modelOptions = options.modelOptions;
    const input = this.convertMessagesToInput(options.messages);
    if (modelOptions) {
      validateTextProviderOptions({
        ...modelOptions,
        model: options.model
      });
    }
    const tools = options.tools ? convertToolsToProviderFormat(options.tools) : void 0;
    const requestParams = {
      model: options.model,
      temperature: options.temperature,
      max_output_tokens: options.maxTokens,
      top_p: options.topP,
      metadata: options.metadata,
      instructions: options.systemPrompts?.join("\n"),
      ...modelOptions,
      input,
      tools
    };
    return requestParams;
  }
  convertMessagesToInput(messages) {
    const result = [];
    for (const message of messages) {
      if (message.role === "tool") {
        result.push({
          type: "function_call_output",
          call_id: message.toolCallId || "",
          output: typeof message.content === "string" ? message.content : JSON.stringify(message.content)
        });
        continue;
      }
      if (message.role === "assistant") {
        if (message.toolCalls && message.toolCalls.length > 0) {
          for (const toolCall of message.toolCalls) {
            const argumentsString = typeof toolCall.function.arguments === "string" ? toolCall.function.arguments : JSON.stringify(toolCall.function.arguments);
            result.push({
              type: "function_call",
              call_id: toolCall.id,
              name: toolCall.function.name,
              arguments: argumentsString
            });
          }
        }
        if (message.content) {
          const contentStr = this.extractTextContent(message.content);
          if (contentStr) {
            result.push({
              type: "message",
              role: "assistant",
              content: contentStr
            });
          }
        }
        continue;
      }
      const contentParts = this.normalizeContent(message.content);
      const openAIContent = [];
      for (const part of contentParts) {
        openAIContent.push(
          this.convertContentPartToOpenAI(
            part
          )
        );
      }
      if (openAIContent.length === 0) {
        openAIContent.push({ type: "input_text", text: "" });
      }
      result.push({
        type: "message",
        role: "user",
        content: openAIContent
      });
    }
    return result;
  }
  /**
   * Converts a ContentPart to OpenAI input content item.
   * Handles text, image, and audio content parts.
   */
  convertContentPartToOpenAI(part) {
    switch (part.type) {
      case "text":
        return {
          type: "input_text",
          text: part.content
        };
      case "image": {
        const imageMetadata = part.metadata;
        if (part.source.type === "url") {
          return {
            type: "input_image",
            image_url: part.source.value,
            detail: imageMetadata?.detail || "auto"
          };
        }
        const imageValue = part.source.value;
        const imageUrl = imageValue.startsWith("data:") ? imageValue : `data:${part.source.mimeType};base64,${imageValue}`;
        return {
          type: "input_image",
          image_url: imageUrl,
          detail: imageMetadata?.detail || "auto"
        };
      }
      case "audio": {
        if (part.source.type === "url") {
          return {
            type: "input_file",
            file_url: part.source.value
          };
        }
        return {
          type: "input_file",
          file_data: part.source.value
        };
      }
      default:
        throw new Error(`Unsupported content part type: ${part.type}`);
    }
  }
  /**
   * Normalizes message content to an array of ContentPart.
   * Handles backward compatibility with string content.
   */
  normalizeContent(content) {
    if (content === null) {
      return [];
    }
    if (typeof content === "string") {
      return [{ type: "text", content }];
    }
    return content;
  }
  /**
   * Extracts text content from a content value that may be string, null, or ContentPart array.
   */
  extractTextContent(content) {
    if (content === null) {
      return "";
    }
    if (typeof content === "string") {
      return content;
    }
    return content.filter((p) => p.type === "text").map((p) => p.content).join("");
  }
}
function createOpenaiChat(model, apiKey, config) {
  return new OpenAITextAdapter({ apiKey, ...config }, model);
}
function openaiText(model, config) {
  const apiKey = getOpenAIApiKeyFromEnv();
  return createOpenaiChat(model, apiKey, config);
}
export {
  OpenAITextAdapter,
  createOpenaiChat,
  openaiText
};
//# sourceMappingURL=text.js.map
