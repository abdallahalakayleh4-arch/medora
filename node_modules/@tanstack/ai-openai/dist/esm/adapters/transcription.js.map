{"version":3,"file":"transcription.js","sources":["../../../src/adapters/transcription.ts"],"sourcesContent":["import { BaseTranscriptionAdapter } from '@tanstack/ai/adapters'\nimport {\n  createOpenAIClient,\n  generateId,\n  getOpenAIApiKeyFromEnv,\n} from '../utils/client'\nimport type { OpenAITranscriptionModel } from '../model-meta'\nimport type { OpenAITranscriptionProviderOptions } from '../audio/transcription-provider-options'\nimport type {\n  TranscriptionOptions,\n  TranscriptionResult,\n  TranscriptionSegment,\n} from '@tanstack/ai'\nimport type OpenAI_SDK from 'openai'\nimport type { OpenAIClientConfig } from '../utils/client'\n\n/**\n * Configuration for OpenAI Transcription adapter\n */\nexport interface OpenAITranscriptionConfig extends OpenAIClientConfig {}\n\n/**\n * OpenAI Transcription (Speech-to-Text) Adapter\n *\n * Tree-shakeable adapter for OpenAI audio transcription functionality.\n * Supports whisper-1, gpt-4o-transcribe, gpt-4o-mini-transcribe, and gpt-4o-transcribe-diarize models.\n *\n * Features:\n * - Multiple transcription models with different capabilities\n * - Language detection or specification\n * - Multiple output formats: json, text, srt, verbose_json, vtt\n * - Word and segment-level timestamps (with verbose_json)\n * - Speaker diarization (with gpt-4o-transcribe-diarize)\n */\nexport class OpenAITranscriptionAdapter<\n  TModel extends OpenAITranscriptionModel,\n> extends BaseTranscriptionAdapter<TModel, OpenAITranscriptionProviderOptions> {\n  readonly name = 'openai' as const\n\n  private client: OpenAI_SDK\n\n  constructor(config: OpenAITranscriptionConfig, model: TModel) {\n    super(config, model)\n    this.client = createOpenAIClient(config)\n  }\n\n  async transcribe(\n    options: TranscriptionOptions<OpenAITranscriptionProviderOptions>,\n  ): Promise<TranscriptionResult> {\n    const { model, audio, language, prompt, responseFormat, modelOptions } =\n      options\n\n    // Convert audio input to File object\n    const file = this.prepareAudioFile(audio)\n\n    // Build request\n    const request: OpenAI_SDK.Audio.TranscriptionCreateParams = {\n      model,\n      file,\n      language,\n      prompt,\n      response_format: this.mapResponseFormat(responseFormat),\n      ...modelOptions,\n    }\n\n    // Call OpenAI API - use verbose_json to get timestamps when available\n    const useVerbose =\n      responseFormat === 'verbose_json' ||\n      (!responseFormat && model !== 'whisper-1')\n\n    if (useVerbose) {\n      const response = await this.client.audio.transcriptions.create({\n        ...request,\n        response_format: 'verbose_json',\n      })\n\n      return {\n        id: generateId(this.name),\n        model,\n        text: response.text,\n        language: response.language,\n        duration: response.duration,\n        segments: response.segments?.map(\n          (seg): TranscriptionSegment => ({\n            id: seg.id,\n            start: seg.start,\n            end: seg.end,\n            text: seg.text,\n            confidence: seg.avg_logprob ? Math.exp(seg.avg_logprob) : undefined,\n          }),\n        ),\n        words: response.words?.map((w) => ({\n          word: w.word,\n          start: w.start,\n          end: w.end,\n        })),\n      }\n    } else {\n      const response = await this.client.audio.transcriptions.create(request)\n\n      return {\n        id: generateId(this.name),\n        model,\n        text: typeof response === 'string' ? response : response.text,\n        language,\n      }\n    }\n  }\n\n  private prepareAudioFile(audio: string | File | Blob | ArrayBuffer): File {\n    // If already a File, return it\n    if (typeof File !== 'undefined' && audio instanceof File) {\n      return audio\n    }\n\n    // If Blob, convert to File\n    if (typeof Blob !== 'undefined' && audio instanceof Blob) {\n      return new File([audio], 'audio.mp3', {\n        type: audio.type || 'audio/mpeg',\n      })\n    }\n\n    // If ArrayBuffer, convert to File\n    if (audio instanceof ArrayBuffer) {\n      return new File([audio], 'audio.mp3', { type: 'audio/mpeg' })\n    }\n\n    // If base64 string, decode and convert to File\n    if (typeof audio === 'string') {\n      // Check if it's a data URL\n      if (audio.startsWith('data:')) {\n        const parts = audio.split(',')\n        const header = parts[0]\n        const base64Data = parts[1] || ''\n        const mimeMatch = header?.match(/data:([^;]+)/)\n        const mimeType = mimeMatch?.[1] || 'audio/mpeg'\n        const binaryStr = atob(base64Data)\n        const bytes = new Uint8Array(binaryStr.length)\n        for (let i = 0; i < binaryStr.length; i++) {\n          bytes[i] = binaryStr.charCodeAt(i)\n        }\n        const extension = mimeType.split('/')[1] || 'mp3'\n        return new File([bytes], `audio.${extension}`, { type: mimeType })\n      }\n\n      // Assume raw base64\n      const binaryStr = atob(audio)\n      const bytes = new Uint8Array(binaryStr.length)\n      for (let i = 0; i < binaryStr.length; i++) {\n        bytes[i] = binaryStr.charCodeAt(i)\n      }\n      return new File([bytes], 'audio.mp3', { type: 'audio/mpeg' })\n    }\n\n    throw new Error('Invalid audio input type')\n  }\n\n  private mapResponseFormat(\n    format?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt',\n  ): OpenAI_SDK.Audio.TranscriptionCreateParams['response_format'] {\n    if (!format) return 'json'\n    return format as OpenAI_SDK.Audio.TranscriptionCreateParams['response_format']\n  }\n}\n\n/**\n * Creates an OpenAI transcription adapter with explicit API key.\n * Type resolution happens here at the call site.\n *\n * @param model - The model name (e.g., 'whisper-1')\n * @param apiKey - Your OpenAI API key\n * @param config - Optional additional configuration\n * @returns Configured OpenAI transcription adapter instance with resolved types\n *\n * @example\n * ```typescript\n * const adapter = createOpenaiTranscription('whisper-1', \"sk-...\");\n *\n * const result = await generateTranscription({\n *   adapter,\n *   audio: audioFile,\n *   language: 'en'\n * });\n * ```\n */\nexport function createOpenaiTranscription<\n  TModel extends OpenAITranscriptionModel,\n>(\n  model: TModel,\n  apiKey: string,\n  config?: Omit<OpenAITranscriptionConfig, 'apiKey'>,\n): OpenAITranscriptionAdapter<TModel> {\n  return new OpenAITranscriptionAdapter({ apiKey, ...config }, model)\n}\n\n/**\n * Creates an OpenAI transcription adapter with automatic API key detection from environment variables.\n * Type resolution happens here at the call site.\n *\n * Looks for `OPENAI_API_KEY` in:\n * - `process.env` (Node.js)\n * - `window.env` (Browser with injected env)\n *\n * @param model - The model name (e.g., 'whisper-1')\n * @param config - Optional configuration (excluding apiKey which is auto-detected)\n * @returns Configured OpenAI transcription adapter instance with resolved types\n * @throws Error if OPENAI_API_KEY is not found in environment\n *\n * @example\n * ```typescript\n * // Automatically uses OPENAI_API_KEY from environment\n * const adapter = openaiTranscription('whisper-1');\n *\n * const result = await generateTranscription({\n *   adapter,\n *   audio: audioFile\n * });\n *\n * console.log(result.text)\n * ```\n */\nexport function openaiTranscription<TModel extends OpenAITranscriptionModel>(\n  model: TModel,\n  config?: Omit<OpenAITranscriptionConfig, 'apiKey'>,\n): OpenAITranscriptionAdapter<TModel> {\n  const apiKey = getOpenAIApiKeyFromEnv()\n  return createOpenaiTranscription(model, apiKey, config)\n}\n"],"names":["binaryStr","bytes"],"mappings":";;AAkCO,MAAM,mCAEH,yBAAqE;AAAA,EAK7E,YAAY,QAAmC,OAAe;AAC5D,UAAM,QAAQ,KAAK;AALrB,SAAS,OAAO;AAMd,SAAK,SAAS,mBAAmB,MAAM;AAAA,EACzC;AAAA,EAEA,MAAM,WACJ,SAC8B;AAC9B,UAAM,EAAE,OAAO,OAAO,UAAU,QAAQ,gBAAgB,iBACtD;AAGF,UAAM,OAAO,KAAK,iBAAiB,KAAK;AAGxC,UAAM,UAAsD;AAAA,MAC1D;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,iBAAiB,KAAK,kBAAkB,cAAc;AAAA,MACtD,GAAG;AAAA,IAAA;AAIL,UAAM,aACJ,mBAAmB,kBAClB,CAAC,kBAAkB,UAAU;AAEhC,QAAI,YAAY;AACd,YAAM,WAAW,MAAM,KAAK,OAAO,MAAM,eAAe,OAAO;AAAA,QAC7D,GAAG;AAAA,QACH,iBAAiB;AAAA,MAAA,CAClB;AAED,aAAO;AAAA,QACL,IAAI,WAAW,KAAK,IAAI;AAAA,QACxB;AAAA,QACA,MAAM,SAAS;AAAA,QACf,UAAU,SAAS;AAAA,QACnB,UAAU,SAAS;AAAA,QACnB,UAAU,SAAS,UAAU;AAAA,UAC3B,CAAC,SAA+B;AAAA,YAC9B,IAAI,IAAI;AAAA,YACR,OAAO,IAAI;AAAA,YACX,KAAK,IAAI;AAAA,YACT,MAAM,IAAI;AAAA,YACV,YAAY,IAAI,cAAc,KAAK,IAAI,IAAI,WAAW,IAAI;AAAA,UAAA;AAAA,QAC5D;AAAA,QAEF,OAAO,SAAS,OAAO,IAAI,CAAC,OAAO;AAAA,UACjC,MAAM,EAAE;AAAA,UACR,OAAO,EAAE;AAAA,UACT,KAAK,EAAE;AAAA,QAAA,EACP;AAAA,MAAA;AAAA,IAEN,OAAO;AACL,YAAM,WAAW,MAAM,KAAK,OAAO,MAAM,eAAe,OAAO,OAAO;AAEtE,aAAO;AAAA,QACL,IAAI,WAAW,KAAK,IAAI;AAAA,QACxB;AAAA,QACA,MAAM,OAAO,aAAa,WAAW,WAAW,SAAS;AAAA,QACzD;AAAA,MAAA;AAAA,IAEJ;AAAA,EACF;AAAA,EAEQ,iBAAiB,OAAiD;AAExE,QAAI,OAAO,SAAS,eAAe,iBAAiB,MAAM;AACxD,aAAO;AAAA,IACT;AAGA,QAAI,OAAO,SAAS,eAAe,iBAAiB,MAAM;AACxD,aAAO,IAAI,KAAK,CAAC,KAAK,GAAG,aAAa;AAAA,QACpC,MAAM,MAAM,QAAQ;AAAA,MAAA,CACrB;AAAA,IACH;AAGA,QAAI,iBAAiB,aAAa;AAChC,aAAO,IAAI,KAAK,CAAC,KAAK,GAAG,aAAa,EAAE,MAAM,cAAc;AAAA,IAC9D;AAGA,QAAI,OAAO,UAAU,UAAU;AAE7B,UAAI,MAAM,WAAW,OAAO,GAAG;AAC7B,cAAM,QAAQ,MAAM,MAAM,GAAG;AAC7B,cAAM,SAAS,MAAM,CAAC;AACtB,cAAM,aAAa,MAAM,CAAC,KAAK;AAC/B,cAAM,YAAY,QAAQ,MAAM,cAAc;AAC9C,cAAM,WAAW,YAAY,CAAC,KAAK;AACnC,cAAMA,aAAY,KAAK,UAAU;AACjC,cAAMC,SAAQ,IAAI,WAAWD,WAAU,MAAM;AAC7C,iBAAS,IAAI,GAAG,IAAIA,WAAU,QAAQ,KAAK;AACzCC,iBAAM,CAAC,IAAID,WAAU,WAAW,CAAC;AAAA,QACnC;AACA,cAAM,YAAY,SAAS,MAAM,GAAG,EAAE,CAAC,KAAK;AAC5C,eAAO,IAAI,KAAK,CAACC,MAAK,GAAG,SAAS,SAAS,IAAI,EAAE,MAAM,UAAU;AAAA,MACnE;AAGA,YAAM,YAAY,KAAK,KAAK;AAC5B,YAAM,QAAQ,IAAI,WAAW,UAAU,MAAM;AAC7C,eAAS,IAAI,GAAG,IAAI,UAAU,QAAQ,KAAK;AACzC,cAAM,CAAC,IAAI,UAAU,WAAW,CAAC;AAAA,MACnC;AACA,aAAO,IAAI,KAAK,CAAC,KAAK,GAAG,aAAa,EAAE,MAAM,cAAc;AAAA,IAC9D;AAEA,UAAM,IAAI,MAAM,0BAA0B;AAAA,EAC5C;AAAA,EAEQ,kBACN,QAC+D;AAC/D,QAAI,CAAC,OAAQ,QAAO;AACpB,WAAO;AAAA,EACT;AACF;AAsBO,SAAS,0BAGd,OACA,QACA,QACoC;AACpC,SAAO,IAAI,2BAA2B,EAAE,QAAQ,GAAG,OAAA,GAAU,KAAK;AACpE;AA4BO,SAAS,oBACd,OACA,QACoC;AACpC,QAAM,SAAS,uBAAA;AACf,SAAO,0BAA0B,OAAO,QAAQ,MAAM;AACxD;"}