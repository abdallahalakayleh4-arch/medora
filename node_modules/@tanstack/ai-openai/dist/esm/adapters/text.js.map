{"version":3,"file":"text.js","sources":["../../../src/adapters/text.ts"],"sourcesContent":["import { BaseTextAdapter } from '@tanstack/ai/adapters'\nimport { validateTextProviderOptions } from '../text/text-provider-options'\nimport { convertToolsToProviderFormat } from '../tools'\nimport {\n  createOpenAIClient,\n  generateId,\n  getOpenAIApiKeyFromEnv,\n} from '../utils/client'\nimport {\n  makeOpenAIStructuredOutputCompatible,\n  transformNullsToUndefined,\n} from '../utils/schema-converter'\nimport type {\n  OPENAI_CHAT_MODELS,\n  OpenAIChatModel,\n  OpenAIChatModelProviderOptionsByName,\n  OpenAIModelInputModalitiesByName,\n} from '../model-meta'\nimport type {\n  StructuredOutputOptions,\n  StructuredOutputResult,\n} from '@tanstack/ai/adapters'\nimport type OpenAI_SDK from 'openai'\nimport type { Responses } from 'openai/resources'\nimport type {\n  ContentPart,\n  ModelMessage,\n  StreamChunk,\n  TextOptions,\n} from '@tanstack/ai'\nimport type {\n  ExternalTextProviderOptions,\n  InternalTextProviderOptions,\n} from '../text/text-provider-options'\nimport type {\n  OpenAIAudioMetadata,\n  OpenAIImageMetadata,\n  OpenAIMessageMetadataByModality,\n} from '../message-types'\nimport type { OpenAIClientConfig } from '../utils/client'\n\n/**\n * Configuration for OpenAI text adapter\n */\nexport interface OpenAITextConfig extends OpenAIClientConfig {}\n\n/**\n * Alias for TextProviderOptions\n */\nexport type OpenAITextProviderOptions = ExternalTextProviderOptions\n\n// ===========================\n// Type Resolution Helpers\n// ===========================\n\n/**\n * Resolve provider options for a specific model.\n * If the model has explicit options in the map, use those; otherwise use base options.\n */\ntype ResolveProviderOptions<TModel extends string> =\n  TModel extends keyof OpenAIChatModelProviderOptionsByName\n    ? OpenAIChatModelProviderOptionsByName[TModel]\n    : OpenAITextProviderOptions\n\n/**\n * Resolve input modalities for a specific model.\n * If the model has explicit modalities in the map, use those; otherwise use all modalities.\n */\ntype ResolveInputModalities<TModel extends string> =\n  TModel extends keyof OpenAIModelInputModalitiesByName\n    ? OpenAIModelInputModalitiesByName[TModel]\n    : readonly ['text', 'image', 'audio']\n\n// ===========================\n// Adapter Implementation\n// ===========================\n\n/**\n * OpenAI Text (Chat) Adapter\n *\n * Tree-shakeable adapter for OpenAI chat/text completion functionality.\n * Import only what you need for smaller bundle sizes.\n */\nexport class OpenAITextAdapter<\n  TModel extends OpenAIChatModel,\n> extends BaseTextAdapter<\n  TModel,\n  ResolveProviderOptions<TModel>,\n  ResolveInputModalities<TModel>,\n  OpenAIMessageMetadataByModality\n> {\n  readonly kind = 'text' as const\n  readonly name = 'openai' as const\n\n  private client: OpenAI_SDK\n\n  constructor(config: OpenAITextConfig, model: TModel) {\n    super({}, model)\n    this.client = createOpenAIClient(config)\n  }\n\n  async *chatStream(\n    options: TextOptions<ResolveProviderOptions<TModel>>,\n  ): AsyncIterable<StreamChunk> {\n    // Track tool call metadata by unique ID\n    // OpenAI streams tool calls with deltas - first chunk has ID/name, subsequent chunks only have args\n    // We assign our own indices as we encounter unique tool call IDs\n    const toolCallMetadata = new Map<\n      string,\n      { index: number; name: string; started: boolean }\n    >()\n    const requestArguments = this.mapTextOptionsToOpenAI(options)\n\n    try {\n      const response = await this.client.responses.create(\n        {\n          ...requestArguments,\n          stream: true,\n        },\n        {\n          headers: options.request?.headers,\n          signal: options.request?.signal,\n        },\n      )\n\n      // Chat Completions API uses SSE format - iterate directly\n      yield* this.processOpenAIStreamChunks(\n        response,\n        toolCallMetadata,\n        options,\n        () => generateId(this.name),\n      )\n    } catch (error: unknown) {\n      const err = error as Error\n      console.error('>>> chatStream: Fatal error during response creation <<<')\n      console.error('>>> Error message:', err.message)\n      console.error('>>> Error stack:', err.stack)\n      console.error('>>> Full error:', err)\n      throw error\n    }\n  }\n\n  /**\n   * Generate structured output using OpenAI's native JSON Schema response format.\n   * Uses stream: false to get the complete response in one call.\n   *\n   * OpenAI has strict requirements for structured output:\n   * - All properties must be in the `required` array\n   * - Optional fields should have null added to their type union\n   * - additionalProperties must be false for all objects\n   *\n   * The outputSchema is already JSON Schema (converted in the ai layer).\n   * We apply OpenAI-specific transformations for structured output compatibility.\n   */\n  async structuredOutput(\n    options: StructuredOutputOptions<ResolveProviderOptions<TModel>>,\n  ): Promise<StructuredOutputResult<unknown>> {\n    const { chatOptions, outputSchema } = options\n    const requestArguments = this.mapTextOptionsToOpenAI(chatOptions)\n\n    // Apply OpenAI-specific transformations for structured output compatibility\n    const jsonSchema = makeOpenAIStructuredOutputCompatible(\n      outputSchema,\n      outputSchema.required || [],\n    )\n\n    try {\n      const response = await this.client.responses.create(\n        {\n          ...requestArguments,\n          stream: false,\n          // Configure structured output via text.format\n          text: {\n            format: {\n              type: 'json_schema',\n              name: 'structured_output',\n              schema: jsonSchema,\n              strict: true,\n            },\n          },\n        },\n        {\n          headers: chatOptions.request?.headers,\n          signal: chatOptions.request?.signal,\n        },\n      )\n\n      // Extract text content from the response\n      const rawText = this.extractTextFromResponse(response)\n\n      // Parse the JSON response\n      let parsed: unknown\n      try {\n        parsed = JSON.parse(rawText)\n      } catch {\n        throw new Error(\n          `Failed to parse structured output as JSON. Content: ${rawText.slice(0, 200)}${rawText.length > 200 ? '...' : ''}`,\n        )\n      }\n\n      // Transform null values to undefined to match original Zod schema expectations\n      // OpenAI returns null for optional fields we made nullable in the schema\n      const transformed = transformNullsToUndefined(parsed)\n\n      return {\n        data: transformed,\n        rawText,\n      }\n    } catch (error: unknown) {\n      const err = error as Error\n      console.error('>>> structuredOutput: Error during response creation <<<')\n      console.error('>>> Error message:', err.message)\n      throw error\n    }\n  }\n\n  /**\n   * Extract text content from a non-streaming response\n   */\n  private extractTextFromResponse(\n    response: OpenAI_SDK.Responses.Response,\n  ): string {\n    let textContent = ''\n\n    for (const item of response.output) {\n      if (item.type === 'message') {\n        for (const part of item.content) {\n          if (part.type === 'output_text') {\n            textContent += part.text\n          }\n        }\n      }\n    }\n\n    return textContent\n  }\n\n  private async *processOpenAIStreamChunks(\n    stream: AsyncIterable<OpenAI_SDK.Responses.ResponseStreamEvent>,\n    toolCallMetadata: Map<\n      string,\n      { index: number; name: string; started: boolean }\n    >,\n    options: TextOptions,\n    genId: () => string,\n  ): AsyncIterable<StreamChunk> {\n    let accumulatedContent = ''\n    let accumulatedReasoning = ''\n    const timestamp = Date.now()\n    let chunkCount = 0\n\n    // Track if we've been streaming deltas to avoid duplicating content from done events\n    let hasStreamedContentDeltas = false\n    let hasStreamedReasoningDeltas = false\n\n    // Preserve response metadata across events\n    let model: string = options.model\n\n    // AG-UI lifecycle tracking\n    const runId = genId()\n    const messageId = genId()\n    let stepId: string | null = null\n    let hasEmittedRunStarted = false\n    let hasEmittedTextMessageStart = false\n    let hasEmittedStepStarted = false\n\n    try {\n      for await (const chunk of stream) {\n        chunkCount++\n\n        // Emit RUN_STARTED on first chunk\n        if (!hasEmittedRunStarted) {\n          hasEmittedRunStarted = true\n          yield {\n            type: 'RUN_STARTED',\n            runId,\n            model: model || options.model,\n            timestamp,\n          }\n        }\n\n        const handleContentPart = (\n          contentPart:\n            | OpenAI_SDK.Responses.ResponseOutputText\n            | OpenAI_SDK.Responses.ResponseOutputRefusal\n            | OpenAI_SDK.Responses.ResponseContentPartAddedEvent.ReasoningText,\n        ): StreamChunk => {\n          if (contentPart.type === 'output_text') {\n            accumulatedContent += contentPart.text\n            return {\n              type: 'TEXT_MESSAGE_CONTENT',\n              messageId,\n              model: model || options.model,\n              timestamp,\n              delta: contentPart.text,\n              content: accumulatedContent,\n            }\n          }\n\n          if (contentPart.type === 'reasoning_text') {\n            accumulatedReasoning += contentPart.text\n            return {\n              type: 'STEP_FINISHED',\n              stepId: stepId || genId(),\n              model: model || options.model,\n              timestamp,\n              delta: contentPart.text,\n              content: accumulatedReasoning,\n            }\n          }\n          return {\n            type: 'RUN_ERROR',\n            runId,\n            model: model || options.model,\n            timestamp,\n            error: {\n              message: contentPart.refusal,\n            },\n          }\n        }\n        // handle general response events\n        if (\n          chunk.type === 'response.created' ||\n          chunk.type === 'response.incomplete' ||\n          chunk.type === 'response.failed'\n        ) {\n          model = chunk.response.model\n          // Reset streaming flags for new response\n          hasStreamedContentDeltas = false\n          hasStreamedReasoningDeltas = false\n          hasEmittedTextMessageStart = false\n          hasEmittedStepStarted = false\n          accumulatedContent = ''\n          accumulatedReasoning = ''\n          if (chunk.response.error) {\n            yield {\n              type: 'RUN_ERROR',\n              runId,\n              model: chunk.response.model,\n              timestamp,\n              error: chunk.response.error,\n            }\n          }\n          if (chunk.response.incomplete_details) {\n            yield {\n              type: 'RUN_ERROR',\n              runId,\n              model: chunk.response.model,\n              timestamp,\n              error: {\n                message: chunk.response.incomplete_details.reason ?? '',\n              },\n            }\n          }\n        }\n        // Handle output text deltas (token-by-token streaming)\n        // response.output_text.delta provides incremental text updates\n        if (chunk.type === 'response.output_text.delta' && chunk.delta) {\n          // Delta can be an array of strings or a single string\n          const textDelta = Array.isArray(chunk.delta)\n            ? chunk.delta.join('')\n            : typeof chunk.delta === 'string'\n              ? chunk.delta\n              : ''\n\n          if (textDelta) {\n            // Emit TEXT_MESSAGE_START on first text content\n            if (!hasEmittedTextMessageStart) {\n              hasEmittedTextMessageStart = true\n              yield {\n                type: 'TEXT_MESSAGE_START',\n                messageId,\n                model: model || options.model,\n                timestamp,\n                role: 'assistant',\n              }\n            }\n\n            accumulatedContent += textDelta\n            hasStreamedContentDeltas = true\n            yield {\n              type: 'TEXT_MESSAGE_CONTENT',\n              messageId,\n              model: model || options.model,\n              timestamp,\n              delta: textDelta,\n              content: accumulatedContent,\n            }\n          }\n        }\n\n        // Handle reasoning deltas (token-by-token thinking/reasoning streaming)\n        // response.reasoning_text.delta provides incremental reasoning updates\n        if (chunk.type === 'response.reasoning_text.delta' && chunk.delta) {\n          // Delta can be an array of strings or a single string\n          const reasoningDelta = Array.isArray(chunk.delta)\n            ? chunk.delta.join('')\n            : typeof chunk.delta === 'string'\n              ? chunk.delta\n              : ''\n\n          if (reasoningDelta) {\n            // Emit STEP_STARTED on first reasoning content\n            if (!hasEmittedStepStarted) {\n              hasEmittedStepStarted = true\n              stepId = genId()\n              yield {\n                type: 'STEP_STARTED',\n                stepId,\n                model: model || options.model,\n                timestamp,\n                stepType: 'thinking',\n              }\n            }\n\n            accumulatedReasoning += reasoningDelta\n            hasStreamedReasoningDeltas = true\n            yield {\n              type: 'STEP_FINISHED',\n              stepId: stepId || genId(),\n              model: model || options.model,\n              timestamp,\n              delta: reasoningDelta,\n              content: accumulatedReasoning,\n            }\n          }\n        }\n\n        // Handle reasoning summary deltas (when using reasoning.summary option)\n        // response.reasoning_summary_text.delta provides incremental summary updates\n        if (\n          chunk.type === 'response.reasoning_summary_text.delta' &&\n          chunk.delta\n        ) {\n          const summaryDelta =\n            typeof chunk.delta === 'string' ? chunk.delta : ''\n\n          if (summaryDelta) {\n            // Emit STEP_STARTED on first reasoning content\n            if (!hasEmittedStepStarted) {\n              hasEmittedStepStarted = true\n              stepId = genId()\n              yield {\n                type: 'STEP_STARTED',\n                stepId,\n                model: model || options.model,\n                timestamp,\n                stepType: 'thinking',\n              }\n            }\n\n            accumulatedReasoning += summaryDelta\n            hasStreamedReasoningDeltas = true\n            yield {\n              type: 'STEP_FINISHED',\n              stepId: stepId || genId(),\n              model: model || options.model,\n              timestamp,\n              delta: summaryDelta,\n              content: accumulatedReasoning,\n            }\n          }\n        }\n\n        // handle content_part added events for text, reasoning and refusals\n        if (chunk.type === 'response.content_part.added') {\n          const contentPart = chunk.part\n          // Emit TEXT_MESSAGE_START if this is text content\n          if (\n            contentPart.type === 'output_text' &&\n            !hasEmittedTextMessageStart\n          ) {\n            hasEmittedTextMessageStart = true\n            yield {\n              type: 'TEXT_MESSAGE_START',\n              messageId,\n              model: model || options.model,\n              timestamp,\n              role: 'assistant',\n            }\n          }\n          // Emit STEP_STARTED if this is reasoning content\n          if (contentPart.type === 'reasoning_text' && !hasEmittedStepStarted) {\n            hasEmittedStepStarted = true\n            stepId = genId()\n            yield {\n              type: 'STEP_STARTED',\n              stepId,\n              model: model || options.model,\n              timestamp,\n              stepType: 'thinking',\n            }\n          }\n          yield handleContentPart(contentPart)\n        }\n\n        if (chunk.type === 'response.content_part.done') {\n          const contentPart = chunk.part\n\n          // Skip emitting chunks for content parts that we've already streamed via deltas\n          // The done event is just a completion marker, not new content\n          if (contentPart.type === 'output_text' && hasStreamedContentDeltas) {\n            // Content already accumulated from deltas, skip\n            continue\n          }\n          if (\n            contentPart.type === 'reasoning_text' &&\n            hasStreamedReasoningDeltas\n          ) {\n            // Reasoning already accumulated from deltas, skip\n            continue\n          }\n\n          // Only emit if we haven't been streaming deltas (e.g., for non-streaming responses)\n          yield handleContentPart(contentPart)\n        }\n\n        // handle output_item.added to capture function call metadata (name)\n        if (chunk.type === 'response.output_item.added') {\n          const item = chunk.item\n          if (item.type === 'function_call' && item.id) {\n            // Store the function name for later use\n            if (!toolCallMetadata.has(item.id)) {\n              toolCallMetadata.set(item.id, {\n                index: chunk.output_index,\n                name: item.name || '',\n                started: false,\n              })\n            }\n            // Emit TOOL_CALL_START\n            yield {\n              type: 'TOOL_CALL_START',\n              toolCallId: item.id,\n              toolName: item.name || '',\n              model: model || options.model,\n              timestamp,\n              index: chunk.output_index,\n            }\n            toolCallMetadata.get(item.id)!.started = true\n          }\n        }\n\n        // Handle function call arguments delta (streaming)\n        if (\n          chunk.type === 'response.function_call_arguments.delta' &&\n          chunk.delta\n        ) {\n          const metadata = toolCallMetadata.get(chunk.item_id)\n          yield {\n            type: 'TOOL_CALL_ARGS',\n            toolCallId: chunk.item_id,\n            model: model || options.model,\n            timestamp,\n            delta: chunk.delta,\n            args: metadata ? undefined : chunk.delta, // We don't accumulate here, let caller handle it\n          }\n        }\n\n        if (chunk.type === 'response.function_call_arguments.done') {\n          const { item_id } = chunk\n\n          // Get the function name from metadata (captured in output_item.added)\n          const metadata = toolCallMetadata.get(item_id)\n          const name = metadata?.name || ''\n\n          // Parse arguments\n          let parsedInput: unknown = {}\n          try {\n            parsedInput = chunk.arguments ? JSON.parse(chunk.arguments) : {}\n          } catch {\n            parsedInput = {}\n          }\n\n          yield {\n            type: 'TOOL_CALL_END',\n            toolCallId: item_id,\n            toolName: name,\n            model: model || options.model,\n            timestamp,\n            input: parsedInput,\n          }\n        }\n\n        if (chunk.type === 'response.completed') {\n          // Emit TEXT_MESSAGE_END if we had text content\n          if (hasEmittedTextMessageStart) {\n            yield {\n              type: 'TEXT_MESSAGE_END',\n              messageId,\n              model: model || options.model,\n              timestamp,\n            }\n          }\n\n          // Determine finish reason based on output\n          // If there are function_call items in the output, it's a tool_calls finish\n          const hasFunctionCalls = chunk.response.output.some(\n            (item: unknown) =>\n              (item as { type: string }).type === 'function_call',\n          )\n\n          yield {\n            type: 'RUN_FINISHED',\n            runId,\n            model: model || options.model,\n            timestamp,\n            usage: {\n              promptTokens: chunk.response.usage?.input_tokens || 0,\n              completionTokens: chunk.response.usage?.output_tokens || 0,\n              totalTokens: chunk.response.usage?.total_tokens || 0,\n            },\n            finishReason: hasFunctionCalls ? 'tool_calls' : 'stop',\n          }\n        }\n\n        if (chunk.type === 'error') {\n          yield {\n            type: 'RUN_ERROR',\n            runId,\n            model: model || options.model,\n            timestamp,\n            error: {\n              message: chunk.message,\n              code: chunk.code ?? undefined,\n            },\n          }\n        }\n      }\n    } catch (error: unknown) {\n      const err = error as Error & { code?: string }\n      console.log(\n        '[OpenAI Adapter] Stream ended with error. Event type summary:',\n        {\n          totalChunks: chunkCount,\n          error: err.message,\n        },\n      )\n      yield {\n        type: 'RUN_ERROR',\n        runId,\n        model: options.model,\n        timestamp,\n        error: {\n          message: err.message || 'Unknown error occurred',\n          code: err.code,\n        },\n      }\n    }\n  }\n\n  /**\n   * Maps common options to OpenAI-specific format\n   * Handles translation of normalized options to OpenAI's API format\n   */\n  private mapTextOptionsToOpenAI(options: TextOptions) {\n    const modelOptions = options.modelOptions as\n      | Omit<\n          InternalTextProviderOptions,\n          | 'max_output_tokens'\n          | 'tools'\n          | 'metadata'\n          | 'temperature'\n          | 'input'\n          | 'top_p'\n        >\n      | undefined\n    const input = this.convertMessagesToInput(options.messages)\n    if (modelOptions) {\n      validateTextProviderOptions({\n        ...modelOptions,\n        input,\n        model: options.model,\n      })\n    }\n\n    const tools = options.tools\n      ? convertToolsToProviderFormat(options.tools)\n      : undefined\n\n    const requestParams: Omit<\n      OpenAI_SDK.Responses.ResponseCreateParams,\n      'stream'\n    > = {\n      model: options.model,\n      temperature: options.temperature,\n      max_output_tokens: options.maxTokens,\n      top_p: options.topP,\n      metadata: options.metadata,\n      instructions: options.systemPrompts?.join('\\n'),\n      ...modelOptions,\n      input,\n      tools,\n    }\n\n    return requestParams\n  }\n\n  private convertMessagesToInput(\n    messages: Array<ModelMessage>,\n  ): Responses.ResponseInput {\n    const result: Responses.ResponseInput = []\n\n    for (const message of messages) {\n      // Handle tool messages - convert to FunctionToolCallOutput\n      if (message.role === 'tool') {\n        result.push({\n          type: 'function_call_output',\n          call_id: message.toolCallId || '',\n          output:\n            typeof message.content === 'string'\n              ? message.content\n              : JSON.stringify(message.content),\n        })\n        continue\n      }\n\n      // Handle assistant messages\n      if (message.role === 'assistant') {\n        // If the assistant message has tool calls, add them as FunctionToolCall objects\n        // OpenAI Responses API expects arguments as a string (JSON string)\n        if (message.toolCalls && message.toolCalls.length > 0) {\n          for (const toolCall of message.toolCalls) {\n            // Keep arguments as string for Responses API\n            // Our internal format stores arguments as a JSON string, which is what API expects\n            const argumentsString =\n              typeof toolCall.function.arguments === 'string'\n                ? toolCall.function.arguments\n                : JSON.stringify(toolCall.function.arguments)\n\n            result.push({\n              type: 'function_call',\n              call_id: toolCall.id,\n              name: toolCall.function.name,\n              arguments: argumentsString,\n            })\n          }\n        }\n\n        // Add the assistant's text message if there is content\n        if (message.content) {\n          // Assistant messages are typically text-only\n          const contentStr = this.extractTextContent(message.content)\n          if (contentStr) {\n            result.push({\n              type: 'message',\n              role: 'assistant',\n              content: contentStr,\n            })\n          }\n        }\n\n        continue\n      }\n\n      // Handle user messages (default case) - support multimodal content\n      const contentParts = this.normalizeContent(message.content)\n      const openAIContent: Array<Responses.ResponseInputContent> = []\n\n      for (const part of contentParts) {\n        openAIContent.push(\n          this.convertContentPartToOpenAI(\n            part as ContentPart<\n              unknown,\n              OpenAIImageMetadata,\n              OpenAIAudioMetadata,\n              unknown,\n              unknown\n            >,\n          ),\n        )\n      }\n\n      // If no content parts, add empty text\n      if (openAIContent.length === 0) {\n        openAIContent.push({ type: 'input_text', text: '' })\n      }\n\n      result.push({\n        type: 'message',\n        role: 'user',\n        content: openAIContent,\n      })\n    }\n\n    return result\n  }\n\n  /**\n   * Converts a ContentPart to OpenAI input content item.\n   * Handles text, image, and audio content parts.\n   */\n  private convertContentPartToOpenAI(\n    part: ContentPart<\n      unknown,\n      OpenAIImageMetadata,\n      OpenAIAudioMetadata,\n      unknown,\n      unknown\n    >,\n  ): Responses.ResponseInputContent {\n    switch (part.type) {\n      case 'text':\n        return {\n          type: 'input_text',\n          text: part.content,\n        }\n      case 'image': {\n        const imageMetadata = part.metadata\n        if (part.source.type === 'url') {\n          return {\n            type: 'input_image',\n            image_url: part.source.value,\n            detail: imageMetadata?.detail || 'auto',\n          }\n        }\n        // For base64 data, construct a data URI using the mimeType from source\n        const imageValue = part.source.value\n        const imageUrl = imageValue.startsWith('data:')\n          ? imageValue\n          : `data:${part.source.mimeType};base64,${imageValue}`\n        return {\n          type: 'input_image',\n          image_url: imageUrl,\n          detail: imageMetadata?.detail || 'auto',\n        }\n      }\n      case 'audio': {\n        if (part.source.type === 'url') {\n          // OpenAI may support audio URLs in the future\n          // For now, treat as data URI\n          return {\n            type: 'input_file',\n            file_url: part.source.value,\n          }\n        }\n        return {\n          type: 'input_file',\n          file_data: part.source.value,\n        }\n      }\n\n      default:\n        throw new Error(`Unsupported content part type: ${part.type}`)\n    }\n  }\n\n  /**\n   * Normalizes message content to an array of ContentPart.\n   * Handles backward compatibility with string content.\n   */\n  private normalizeContent(\n    content: string | null | Array<ContentPart>,\n  ): Array<ContentPart> {\n    if (content === null) {\n      return []\n    }\n    if (typeof content === 'string') {\n      return [{ type: 'text', content: content }]\n    }\n    return content\n  }\n\n  /**\n   * Extracts text content from a content value that may be string, null, or ContentPart array.\n   */\n  private extractTextContent(\n    content: string | null | Array<ContentPart>,\n  ): string {\n    if (content === null) {\n      return ''\n    }\n    if (typeof content === 'string') {\n      return content\n    }\n    // It's an array of ContentPart\n    return content\n      .filter((p) => p.type === 'text')\n      .map((p) => p.content)\n      .join('')\n  }\n}\n\n/**\n * Creates an OpenAI chat adapter with explicit API key.\n * Type resolution happens here at the call site.\n *\n * @param model - The model name (e.g., 'gpt-4o', 'gpt-4-turbo')\n * @param apiKey - Your OpenAI API key\n * @param config - Optional additional configuration\n * @returns Configured OpenAI chat adapter instance with resolved types\n *\n * @example\n * ```typescript\n * const adapter = createOpenaiChat('gpt-4o', \"sk-...\");\n * // adapter has type-safe modelOptions for gpt-4o\n * ```\n */\nexport function createOpenaiChat<\n  TModel extends (typeof OPENAI_CHAT_MODELS)[number],\n>(\n  model: TModel,\n  apiKey: string,\n  config?: Omit<OpenAITextConfig, 'apiKey'>,\n): OpenAITextAdapter<TModel> {\n  return new OpenAITextAdapter({ apiKey, ...config }, model)\n}\n\n/**\n * Creates an OpenAI text adapter with automatic API key detection from environment variables.\n * Type resolution happens here at the call site.\n *\n * Looks for `OPENAI_API_KEY` in:\n * - `process.env` (Node.js)\n * - `window.env` (Browser with injected env)\n *\n * @param model - The model name (e.g., 'gpt-4o', 'gpt-4-turbo')\n * @param config - Optional configuration (excluding apiKey which is auto-detected)\n * @returns Configured OpenAI text adapter instance with resolved types\n * @throws Error if OPENAI_API_KEY is not found in environment\n *\n * @example\n * ```typescript\n * // Automatically uses OPENAI_API_KEY from environment\n * const adapter = openaiText('gpt-4o');\n *\n * const stream = chat({\n *   adapter,\n *   messages: [{ role: \"user\", content: \"Hello!\" }]\n * });\n * ```\n */\nexport function openaiText<TModel extends (typeof OPENAI_CHAT_MODELS)[number]>(\n  model: TModel,\n  config?: Omit<OpenAITextConfig, 'apiKey'>,\n): OpenAITextAdapter<TModel> {\n  const apiKey = getOpenAIApiKeyFromEnv()\n  return createOpenaiChat(model, apiKey, config)\n}\n"],"names":[],"mappings":";;;;;AAmFO,MAAM,0BAEH,gBAKR;AAAA,EAMA,YAAY,QAA0B,OAAe;AACnD,UAAM,CAAA,GAAI,KAAK;AANjB,SAAS,OAAO;AAChB,SAAS,OAAO;AAMd,SAAK,SAAS,mBAAmB,MAAM;AAAA,EACzC;AAAA,EAEA,OAAO,WACL,SAC4B;AAI5B,UAAM,uCAAuB,IAAA;AAI7B,UAAM,mBAAmB,KAAK,uBAAuB,OAAO;AAE5D,QAAI;AACF,YAAM,WAAW,MAAM,KAAK,OAAO,UAAU;AAAA,QAC3C;AAAA,UACE,GAAG;AAAA,UACH,QAAQ;AAAA,QAAA;AAAA,QAEV;AAAA,UACE,SAAS,QAAQ,SAAS;AAAA,UAC1B,QAAQ,QAAQ,SAAS;AAAA,QAAA;AAAA,MAC3B;AAIF,aAAO,KAAK;AAAA,QACV;AAAA,QACA;AAAA,QACA;AAAA,QACA,MAAM,WAAW,KAAK,IAAI;AAAA,MAAA;AAAA,IAE9B,SAAS,OAAgB;AACvB,YAAM,MAAM;AACZ,cAAQ,MAAM,0DAA0D;AACxE,cAAQ,MAAM,sBAAsB,IAAI,OAAO;AAC/C,cAAQ,MAAM,oBAAoB,IAAI,KAAK;AAC3C,cAAQ,MAAM,mBAAmB,GAAG;AACpC,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAcA,MAAM,iBACJ,SAC0C;AAC1C,UAAM,EAAE,aAAa,aAAA,IAAiB;AACtC,UAAM,mBAAmB,KAAK,uBAAuB,WAAW;AAGhE,UAAM,aAAa;AAAA,MACjB;AAAA,MACA,aAAa,YAAY,CAAA;AAAA,IAAC;AAG5B,QAAI;AACF,YAAM,WAAW,MAAM,KAAK,OAAO,UAAU;AAAA,QAC3C;AAAA,UACE,GAAG;AAAA,UACH,QAAQ;AAAA;AAAA,UAER,MAAM;AAAA,YACJ,QAAQ;AAAA,cACN,MAAM;AAAA,cACN,MAAM;AAAA,cACN,QAAQ;AAAA,cACR,QAAQ;AAAA,YAAA;AAAA,UACV;AAAA,QACF;AAAA,QAEF;AAAA,UACE,SAAS,YAAY,SAAS;AAAA,UAC9B,QAAQ,YAAY,SAAS;AAAA,QAAA;AAAA,MAC/B;AAIF,YAAM,UAAU,KAAK,wBAAwB,QAAQ;AAGrD,UAAI;AACJ,UAAI;AACF,iBAAS,KAAK,MAAM,OAAO;AAAA,MAC7B,QAAQ;AACN,cAAM,IAAI;AAAA,UACR,uDAAuD,QAAQ,MAAM,GAAG,GAAG,CAAC,GAAG,QAAQ,SAAS,MAAM,QAAQ,EAAE;AAAA,QAAA;AAAA,MAEpH;AAIA,YAAM,cAAc,0BAA0B,MAAM;AAEpD,aAAO;AAAA,QACL,MAAM;AAAA,QACN;AAAA,MAAA;AAAA,IAEJ,SAAS,OAAgB;AACvB,YAAM,MAAM;AACZ,cAAQ,MAAM,0DAA0D;AACxE,cAAQ,MAAM,sBAAsB,IAAI,OAAO;AAC/C,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,wBACN,UACQ;AACR,QAAI,cAAc;AAElB,eAAW,QAAQ,SAAS,QAAQ;AAClC,UAAI,KAAK,SAAS,WAAW;AAC3B,mBAAW,QAAQ,KAAK,SAAS;AAC/B,cAAI,KAAK,SAAS,eAAe;AAC/B,2BAAe,KAAK;AAAA,UACtB;AAAA,QACF;AAAA,MACF;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA,EAEA,OAAe,0BACb,QACA,kBAIA,SACA,OAC4B;AAC5B,QAAI,qBAAqB;AACzB,QAAI,uBAAuB;AAC3B,UAAM,YAAY,KAAK,IAAA;AACvB,QAAI,aAAa;AAGjB,QAAI,2BAA2B;AAC/B,QAAI,6BAA6B;AAGjC,QAAI,QAAgB,QAAQ;AAG5B,UAAM,QAAQ,MAAA;AACd,UAAM,YAAY,MAAA;AAClB,QAAI,SAAwB;AAC5B,QAAI,uBAAuB;AAC3B,QAAI,6BAA6B;AACjC,QAAI,wBAAwB;AAE5B,QAAI;AACF,uBAAiB,SAAS,QAAQ;AAChC;AAGA,YAAI,CAAC,sBAAsB;AACzB,iCAAuB;AACvB,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN;AAAA,YACA,OAAO,SAAS,QAAQ;AAAA,YACxB;AAAA,UAAA;AAAA,QAEJ;AAEA,cAAM,oBAAoB,CACxB,gBAIgB;AAChB,cAAI,YAAY,SAAS,eAAe;AACtC,kCAAsB,YAAY;AAClC,mBAAO;AAAA,cACL,MAAM;AAAA,cACN;AAAA,cACA,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,OAAO,YAAY;AAAA,cACnB,SAAS;AAAA,YAAA;AAAA,UAEb;AAEA,cAAI,YAAY,SAAS,kBAAkB;AACzC,oCAAwB,YAAY;AACpC,mBAAO;AAAA,cACL,MAAM;AAAA,cACN,QAAQ,UAAU,MAAA;AAAA,cAClB,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,OAAO,YAAY;AAAA,cACnB,SAAS;AAAA,YAAA;AAAA,UAEb;AACA,iBAAO;AAAA,YACL,MAAM;AAAA,YACN;AAAA,YACA,OAAO,SAAS,QAAQ;AAAA,YACxB;AAAA,YACA,OAAO;AAAA,cACL,SAAS,YAAY;AAAA,YAAA;AAAA,UACvB;AAAA,QAEJ;AAEA,YACE,MAAM,SAAS,sBACf,MAAM,SAAS,yBACf,MAAM,SAAS,mBACf;AACA,kBAAQ,MAAM,SAAS;AAEvB,qCAA2B;AAC3B,uCAA6B;AAC7B,uCAA6B;AAC7B,kCAAwB;AACxB,+BAAqB;AACrB,iCAAuB;AACvB,cAAI,MAAM,SAAS,OAAO;AACxB,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN;AAAA,cACA,OAAO,MAAM,SAAS;AAAA,cACtB;AAAA,cACA,OAAO,MAAM,SAAS;AAAA,YAAA;AAAA,UAE1B;AACA,cAAI,MAAM,SAAS,oBAAoB;AACrC,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN;AAAA,cACA,OAAO,MAAM,SAAS;AAAA,cACtB;AAAA,cACA,OAAO;AAAA,gBACL,SAAS,MAAM,SAAS,mBAAmB,UAAU;AAAA,cAAA;AAAA,YACvD;AAAA,UAEJ;AAAA,QACF;AAGA,YAAI,MAAM,SAAS,gCAAgC,MAAM,OAAO;AAE9D,gBAAM,YAAY,MAAM,QAAQ,MAAM,KAAK,IACvC,MAAM,MAAM,KAAK,EAAE,IACnB,OAAO,MAAM,UAAU,WACrB,MAAM,QACN;AAEN,cAAI,WAAW;AAEb,gBAAI,CAAC,4BAA4B;AAC/B,2CAA6B;AAC7B,oBAAM;AAAA,gBACJ,MAAM;AAAA,gBACN;AAAA,gBACA,OAAO,SAAS,QAAQ;AAAA,gBACxB;AAAA,gBACA,MAAM;AAAA,cAAA;AAAA,YAEV;AAEA,kCAAsB;AACtB,uCAA2B;AAC3B,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN;AAAA,cACA,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,OAAO;AAAA,cACP,SAAS;AAAA,YAAA;AAAA,UAEb;AAAA,QACF;AAIA,YAAI,MAAM,SAAS,mCAAmC,MAAM,OAAO;AAEjE,gBAAM,iBAAiB,MAAM,QAAQ,MAAM,KAAK,IAC5C,MAAM,MAAM,KAAK,EAAE,IACnB,OAAO,MAAM,UAAU,WACrB,MAAM,QACN;AAEN,cAAI,gBAAgB;AAElB,gBAAI,CAAC,uBAAuB;AAC1B,sCAAwB;AACxB,uBAAS,MAAA;AACT,oBAAM;AAAA,gBACJ,MAAM;AAAA,gBACN;AAAA,gBACA,OAAO,SAAS,QAAQ;AAAA,gBACxB;AAAA,gBACA,UAAU;AAAA,cAAA;AAAA,YAEd;AAEA,oCAAwB;AACxB,yCAA6B;AAC7B,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN,QAAQ,UAAU,MAAA;AAAA,cAClB,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,OAAO;AAAA,cACP,SAAS;AAAA,YAAA;AAAA,UAEb;AAAA,QACF;AAIA,YACE,MAAM,SAAS,2CACf,MAAM,OACN;AACA,gBAAM,eACJ,OAAO,MAAM,UAAU,WAAW,MAAM,QAAQ;AAElD,cAAI,cAAc;AAEhB,gBAAI,CAAC,uBAAuB;AAC1B,sCAAwB;AACxB,uBAAS,MAAA;AACT,oBAAM;AAAA,gBACJ,MAAM;AAAA,gBACN;AAAA,gBACA,OAAO,SAAS,QAAQ;AAAA,gBACxB;AAAA,gBACA,UAAU;AAAA,cAAA;AAAA,YAEd;AAEA,oCAAwB;AACxB,yCAA6B;AAC7B,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN,QAAQ,UAAU,MAAA;AAAA,cAClB,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,OAAO;AAAA,cACP,SAAS;AAAA,YAAA;AAAA,UAEb;AAAA,QACF;AAGA,YAAI,MAAM,SAAS,+BAA+B;AAChD,gBAAM,cAAc,MAAM;AAE1B,cACE,YAAY,SAAS,iBACrB,CAAC,4BACD;AACA,yCAA6B;AAC7B,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN;AAAA,cACA,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,MAAM;AAAA,YAAA;AAAA,UAEV;AAEA,cAAI,YAAY,SAAS,oBAAoB,CAAC,uBAAuB;AACnE,oCAAwB;AACxB,qBAAS,MAAA;AACT,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN;AAAA,cACA,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,UAAU;AAAA,YAAA;AAAA,UAEd;AACA,gBAAM,kBAAkB,WAAW;AAAA,QACrC;AAEA,YAAI,MAAM,SAAS,8BAA8B;AAC/C,gBAAM,cAAc,MAAM;AAI1B,cAAI,YAAY,SAAS,iBAAiB,0BAA0B;AAElE;AAAA,UACF;AACA,cACE,YAAY,SAAS,oBACrB,4BACA;AAEA;AAAA,UACF;AAGA,gBAAM,kBAAkB,WAAW;AAAA,QACrC;AAGA,YAAI,MAAM,SAAS,8BAA8B;AAC/C,gBAAM,OAAO,MAAM;AACnB,cAAI,KAAK,SAAS,mBAAmB,KAAK,IAAI;AAE5C,gBAAI,CAAC,iBAAiB,IAAI,KAAK,EAAE,GAAG;AAClC,+BAAiB,IAAI,KAAK,IAAI;AAAA,gBAC5B,OAAO,MAAM;AAAA,gBACb,MAAM,KAAK,QAAQ;AAAA,gBACnB,SAAS;AAAA,cAAA,CACV;AAAA,YACH;AAEA,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN,YAAY,KAAK;AAAA,cACjB,UAAU,KAAK,QAAQ;AAAA,cACvB,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,cACA,OAAO,MAAM;AAAA,YAAA;AAEf,6BAAiB,IAAI,KAAK,EAAE,EAAG,UAAU;AAAA,UAC3C;AAAA,QACF;AAGA,YACE,MAAM,SAAS,4CACf,MAAM,OACN;AACA,gBAAM,WAAW,iBAAiB,IAAI,MAAM,OAAO;AACnD,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN,YAAY,MAAM;AAAA,YAClB,OAAO,SAAS,QAAQ;AAAA,YACxB;AAAA,YACA,OAAO,MAAM;AAAA,YACb,MAAM,WAAW,SAAY,MAAM;AAAA;AAAA,UAAA;AAAA,QAEvC;AAEA,YAAI,MAAM,SAAS,yCAAyC;AAC1D,gBAAM,EAAE,YAAY;AAGpB,gBAAM,WAAW,iBAAiB,IAAI,OAAO;AAC7C,gBAAM,OAAO,UAAU,QAAQ;AAG/B,cAAI,cAAuB,CAAA;AAC3B,cAAI;AACF,0BAAc,MAAM,YAAY,KAAK,MAAM,MAAM,SAAS,IAAI,CAAA;AAAA,UAChE,QAAQ;AACN,0BAAc,CAAA;AAAA,UAChB;AAEA,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN,YAAY;AAAA,YACZ,UAAU;AAAA,YACV,OAAO,SAAS,QAAQ;AAAA,YACxB;AAAA,YACA,OAAO;AAAA,UAAA;AAAA,QAEX;AAEA,YAAI,MAAM,SAAS,sBAAsB;AAEvC,cAAI,4BAA4B;AAC9B,kBAAM;AAAA,cACJ,MAAM;AAAA,cACN;AAAA,cACA,OAAO,SAAS,QAAQ;AAAA,cACxB;AAAA,YAAA;AAAA,UAEJ;AAIA,gBAAM,mBAAmB,MAAM,SAAS,OAAO;AAAA,YAC7C,CAAC,SACE,KAA0B,SAAS;AAAA,UAAA;AAGxC,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN;AAAA,YACA,OAAO,SAAS,QAAQ;AAAA,YACxB;AAAA,YACA,OAAO;AAAA,cACL,cAAc,MAAM,SAAS,OAAO,gBAAgB;AAAA,cACpD,kBAAkB,MAAM,SAAS,OAAO,iBAAiB;AAAA,cACzD,aAAa,MAAM,SAAS,OAAO,gBAAgB;AAAA,YAAA;AAAA,YAErD,cAAc,mBAAmB,eAAe;AAAA,UAAA;AAAA,QAEpD;AAEA,YAAI,MAAM,SAAS,SAAS;AAC1B,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN;AAAA,YACA,OAAO,SAAS,QAAQ;AAAA,YACxB;AAAA,YACA,OAAO;AAAA,cACL,SAAS,MAAM;AAAA,cACf,MAAM,MAAM,QAAQ;AAAA,YAAA;AAAA,UACtB;AAAA,QAEJ;AAAA,MACF;AAAA,IACF,SAAS,OAAgB;AACvB,YAAM,MAAM;AACZ,cAAQ;AAAA,QACN;AAAA,QACA;AAAA,UACE,aAAa;AAAA,UACb,OAAO,IAAI;AAAA,QAAA;AAAA,MACb;AAEF,YAAM;AAAA,QACJ,MAAM;AAAA,QACN;AAAA,QACA,OAAO,QAAQ;AAAA,QACf;AAAA,QACA,OAAO;AAAA,UACL,SAAS,IAAI,WAAW;AAAA,UACxB,MAAM,IAAI;AAAA,QAAA;AAAA,MACZ;AAAA,IAEJ;AAAA,EACF;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,uBAAuB,SAAsB;AACnD,UAAM,eAAe,QAAQ;AAW7B,UAAM,QAAQ,KAAK,uBAAuB,QAAQ,QAAQ;AAC1D,QAAI,cAAc;AAChB,kCAA4B;AAAA,QAC1B,GAAG;AAAA,QAEH,OAAO,QAAQ;AAAA,MAAA,CAChB;AAAA,IACH;AAEA,UAAM,QAAQ,QAAQ,QAClB,6BAA6B,QAAQ,KAAK,IAC1C;AAEJ,UAAM,gBAGF;AAAA,MACF,OAAO,QAAQ;AAAA,MACf,aAAa,QAAQ;AAAA,MACrB,mBAAmB,QAAQ;AAAA,MAC3B,OAAO,QAAQ;AAAA,MACf,UAAU,QAAQ;AAAA,MAClB,cAAc,QAAQ,eAAe,KAAK,IAAI;AAAA,MAC9C,GAAG;AAAA,MACH;AAAA,MACA;AAAA,IAAA;AAGF,WAAO;AAAA,EACT;AAAA,EAEQ,uBACN,UACyB;AACzB,UAAM,SAAkC,CAAA;AAExC,eAAW,WAAW,UAAU;AAE9B,UAAI,QAAQ,SAAS,QAAQ;AAC3B,eAAO,KAAK;AAAA,UACV,MAAM;AAAA,UACN,SAAS,QAAQ,cAAc;AAAA,UAC/B,QACE,OAAO,QAAQ,YAAY,WACvB,QAAQ,UACR,KAAK,UAAU,QAAQ,OAAO;AAAA,QAAA,CACrC;AACD;AAAA,MACF;AAGA,UAAI,QAAQ,SAAS,aAAa;AAGhC,YAAI,QAAQ,aAAa,QAAQ,UAAU,SAAS,GAAG;AACrD,qBAAW,YAAY,QAAQ,WAAW;AAGxC,kBAAM,kBACJ,OAAO,SAAS,SAAS,cAAc,WACnC,SAAS,SAAS,YAClB,KAAK,UAAU,SAAS,SAAS,SAAS;AAEhD,mBAAO,KAAK;AAAA,cACV,MAAM;AAAA,cACN,SAAS,SAAS;AAAA,cAClB,MAAM,SAAS,SAAS;AAAA,cACxB,WAAW;AAAA,YAAA,CACZ;AAAA,UACH;AAAA,QACF;AAGA,YAAI,QAAQ,SAAS;AAEnB,gBAAM,aAAa,KAAK,mBAAmB,QAAQ,OAAO;AAC1D,cAAI,YAAY;AACd,mBAAO,KAAK;AAAA,cACV,MAAM;AAAA,cACN,MAAM;AAAA,cACN,SAAS;AAAA,YAAA,CACV;AAAA,UACH;AAAA,QACF;AAEA;AAAA,MACF;AAGA,YAAM,eAAe,KAAK,iBAAiB,QAAQ,OAAO;AAC1D,YAAM,gBAAuD,CAAA;AAE7D,iBAAW,QAAQ,cAAc;AAC/B,sBAAc;AAAA,UACZ,KAAK;AAAA,YACH;AAAA,UAAA;AAAA,QAOF;AAAA,MAEJ;AAGA,UAAI,cAAc,WAAW,GAAG;AAC9B,sBAAc,KAAK,EAAE,MAAM,cAAc,MAAM,IAAI;AAAA,MACrD;AAEA,aAAO,KAAK;AAAA,QACV,MAAM;AAAA,QACN,MAAM;AAAA,QACN,SAAS;AAAA,MAAA,CACV;AAAA,IACH;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,2BACN,MAOgC;AAChC,YAAQ,KAAK,MAAA;AAAA,MACX,KAAK;AACH,eAAO;AAAA,UACL,MAAM;AAAA,UACN,MAAM,KAAK;AAAA,QAAA;AAAA,MAEf,KAAK,SAAS;AACZ,cAAM,gBAAgB,KAAK;AAC3B,YAAI,KAAK,OAAO,SAAS,OAAO;AAC9B,iBAAO;AAAA,YACL,MAAM;AAAA,YACN,WAAW,KAAK,OAAO;AAAA,YACvB,QAAQ,eAAe,UAAU;AAAA,UAAA;AAAA,QAErC;AAEA,cAAM,aAAa,KAAK,OAAO;AAC/B,cAAM,WAAW,WAAW,WAAW,OAAO,IAC1C,aACA,QAAQ,KAAK,OAAO,QAAQ,WAAW,UAAU;AACrD,eAAO;AAAA,UACL,MAAM;AAAA,UACN,WAAW;AAAA,UACX,QAAQ,eAAe,UAAU;AAAA,QAAA;AAAA,MAErC;AAAA,MACA,KAAK,SAAS;AACZ,YAAI,KAAK,OAAO,SAAS,OAAO;AAG9B,iBAAO;AAAA,YACL,MAAM;AAAA,YACN,UAAU,KAAK,OAAO;AAAA,UAAA;AAAA,QAE1B;AACA,eAAO;AAAA,UACL,MAAM;AAAA,UACN,WAAW,KAAK,OAAO;AAAA,QAAA;AAAA,MAE3B;AAAA,MAEA;AACE,cAAM,IAAI,MAAM,kCAAkC,KAAK,IAAI,EAAE;AAAA,IAAA;AAAA,EAEnE;AAAA;AAAA;AAAA;AAAA;AAAA,EAMQ,iBACN,SACoB;AACpB,QAAI,YAAY,MAAM;AACpB,aAAO,CAAA;AAAA,IACT;AACA,QAAI,OAAO,YAAY,UAAU;AAC/B,aAAO,CAAC,EAAE,MAAM,QAAQ,SAAkB;AAAA,IAC5C;AACA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,mBACN,SACQ;AACR,QAAI,YAAY,MAAM;AACpB,aAAO;AAAA,IACT;AACA,QAAI,OAAO,YAAY,UAAU;AAC/B,aAAO;AAAA,IACT;AAEA,WAAO,QACJ,OAAO,CAAC,MAAM,EAAE,SAAS,MAAM,EAC/B,IAAI,CAAC,MAAM,EAAE,OAAO,EACpB,KAAK,EAAE;AAAA,EACZ;AACF;AAiBO,SAAS,iBAGd,OACA,QACA,QAC2B;AAC3B,SAAO,IAAI,kBAAkB,EAAE,QAAQ,GAAG,OAAA,GAAU,KAAK;AAC3D;AA0BO,SAAS,WACd,OACA,QAC2B;AAC3B,QAAM,SAAS,uBAAA;AACf,SAAO,iBAAiB,OAAO,QAAQ,MAAM;AAC/C;"}