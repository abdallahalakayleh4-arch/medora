import { BaseTextAdapter, StructuredOutputOptions, StructuredOutputResult } from '@tanstack/ai/adapters';
import { OPENAI_CHAT_MODELS, OpenAIChatModel, OpenAIChatModelProviderOptionsByName, OpenAIModelInputModalitiesByName } from '../model-meta.js';
import { StreamChunk, TextOptions } from '@tanstack/ai';
import { ExternalTextProviderOptions } from '../text/text-provider-options.js';
import { OpenAIMessageMetadataByModality } from '../message-types.js';
import { OpenAIClientConfig } from '../utils/client.js';
/**
 * Configuration for OpenAI text adapter
 */
export interface OpenAITextConfig extends OpenAIClientConfig {
}
/**
 * Alias for TextProviderOptions
 */
export type OpenAITextProviderOptions = ExternalTextProviderOptions;
/**
 * Resolve provider options for a specific model.
 * If the model has explicit options in the map, use those; otherwise use base options.
 */
type ResolveProviderOptions<TModel extends string> = TModel extends keyof OpenAIChatModelProviderOptionsByName ? OpenAIChatModelProviderOptionsByName[TModel] : OpenAITextProviderOptions;
/**
 * Resolve input modalities for a specific model.
 * If the model has explicit modalities in the map, use those; otherwise use all modalities.
 */
type ResolveInputModalities<TModel extends string> = TModel extends keyof OpenAIModelInputModalitiesByName ? OpenAIModelInputModalitiesByName[TModel] : readonly ['text', 'image', 'audio'];
/**
 * OpenAI Text (Chat) Adapter
 *
 * Tree-shakeable adapter for OpenAI chat/text completion functionality.
 * Import only what you need for smaller bundle sizes.
 */
export declare class OpenAITextAdapter<TModel extends OpenAIChatModel> extends BaseTextAdapter<TModel, ResolveProviderOptions<TModel>, ResolveInputModalities<TModel>, OpenAIMessageMetadataByModality> {
    readonly kind: "text";
    readonly name: "openai";
    private client;
    constructor(config: OpenAITextConfig, model: TModel);
    chatStream(options: TextOptions<ResolveProviderOptions<TModel>>): AsyncIterable<StreamChunk>;
    /**
     * Generate structured output using OpenAI's native JSON Schema response format.
     * Uses stream: false to get the complete response in one call.
     *
     * OpenAI has strict requirements for structured output:
     * - All properties must be in the `required` array
     * - Optional fields should have null added to their type union
     * - additionalProperties must be false for all objects
     *
     * The outputSchema is already JSON Schema (converted in the ai layer).
     * We apply OpenAI-specific transformations for structured output compatibility.
     */
    structuredOutput(options: StructuredOutputOptions<ResolveProviderOptions<TModel>>): Promise<StructuredOutputResult<unknown>>;
    /**
     * Extract text content from a non-streaming response
     */
    private extractTextFromResponse;
    private processOpenAIStreamChunks;
    /**
     * Maps common options to OpenAI-specific format
     * Handles translation of normalized options to OpenAI's API format
     */
    private mapTextOptionsToOpenAI;
    private convertMessagesToInput;
    /**
     * Converts a ContentPart to OpenAI input content item.
     * Handles text, image, and audio content parts.
     */
    private convertContentPartToOpenAI;
    /**
     * Normalizes message content to an array of ContentPart.
     * Handles backward compatibility with string content.
     */
    private normalizeContent;
    /**
     * Extracts text content from a content value that may be string, null, or ContentPart array.
     */
    private extractTextContent;
}
/**
 * Creates an OpenAI chat adapter with explicit API key.
 * Type resolution happens here at the call site.
 *
 * @param model - The model name (e.g., 'gpt-4o', 'gpt-4-turbo')
 * @param apiKey - Your OpenAI API key
 * @param config - Optional additional configuration
 * @returns Configured OpenAI chat adapter instance with resolved types
 *
 * @example
 * ```typescript
 * const adapter = createOpenaiChat('gpt-4o', "sk-...");
 * // adapter has type-safe modelOptions for gpt-4o
 * ```
 */
export declare function createOpenaiChat<TModel extends (typeof OPENAI_CHAT_MODELS)[number]>(model: TModel, apiKey: string, config?: Omit<OpenAITextConfig, 'apiKey'>): OpenAITextAdapter<TModel>;
/**
 * Creates an OpenAI text adapter with automatic API key detection from environment variables.
 * Type resolution happens here at the call site.
 *
 * Looks for `OPENAI_API_KEY` in:
 * - `process.env` (Node.js)
 * - `window.env` (Browser with injected env)
 *
 * @param model - The model name (e.g., 'gpt-4o', 'gpt-4-turbo')
 * @param config - Optional configuration (excluding apiKey which is auto-detected)
 * @returns Configured OpenAI text adapter instance with resolved types
 * @throws Error if OPENAI_API_KEY is not found in environment
 *
 * @example
 * ```typescript
 * // Automatically uses OPENAI_API_KEY from environment
 * const adapter = openaiText('gpt-4o');
 *
 * const stream = chat({
 *   adapter,
 *   messages: [{ role: "user", content: "Hello!" }]
 * });
 * ```
 */
export declare function openaiText<TModel extends (typeof OPENAI_CHAT_MODELS)[number]>(model: TModel, config?: Omit<OpenAITextConfig, 'apiKey'>): OpenAITextAdapter<TModel>;
export {};
