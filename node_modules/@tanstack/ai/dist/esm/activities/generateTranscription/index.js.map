{"version":3,"file":"index.js","sources":["../../../../src/activities/generateTranscription/index.ts"],"sourcesContent":["/**\n * Transcription Activity\n *\n * Transcribes audio to text using speech-to-text models.\n * This is a self-contained module with implementation, types, and JSDoc.\n */\n\nimport { aiEventClient } from '../../event-client.js'\nimport type { TranscriptionAdapter } from './adapter'\nimport type { TranscriptionResult } from '../../types'\n\n// ===========================\n// Activity Kind\n// ===========================\n\n/** The adapter kind this activity handles */\nexport const kind = 'transcription' as const\n\n// ===========================\n// Type Extraction Helpers\n// ===========================\n\n/**\n * Extract provider options from a TranscriptionAdapter via ~types.\n */\nexport type TranscriptionProviderOptions<TAdapter> =\n  TAdapter extends TranscriptionAdapter<any, any>\n    ? TAdapter['~types']['providerOptions']\n    : object\n\n// ===========================\n// Activity Options Type\n// ===========================\n\n/**\n * Options for the transcription activity.\n * The model is extracted from the adapter's model property.\n *\n * @template TAdapter - The transcription adapter type\n */\nexport interface TranscriptionActivityOptions<\n  TAdapter extends TranscriptionAdapter<string, object>,\n> {\n  /** The transcription adapter to use (must be created with a model) */\n  adapter: TAdapter & { kind: typeof kind }\n  /** The audio data to transcribe - can be base64 string, File, Blob, or Buffer */\n  audio: string | File | Blob | ArrayBuffer\n  /** The language of the audio in ISO-639-1 format (e.g., 'en') */\n  language?: string\n  /** An optional prompt to guide the transcription */\n  prompt?: string\n  /** The format of the transcription output */\n  responseFormat?: 'json' | 'text' | 'srt' | 'verbose_json' | 'vtt'\n  /** Provider-specific options for transcription */\n  modelOptions?: TranscriptionProviderOptions<TAdapter>\n}\n\n// ===========================\n// Activity Result Type\n// ===========================\n\n/** Result type for the transcription activity */\nexport type TranscriptionActivityResult = Promise<TranscriptionResult>\n\nfunction createId(prefix: string): string {\n  return `${prefix}-${Date.now()}-${Math.random().toString(36).slice(2, 9)}`\n}\n\n// ===========================\n// Activity Implementation\n// ===========================\n\n/**\n * Transcription activity - converts audio to text.\n *\n * Uses AI speech-to-text models to transcribe audio content.\n *\n * @example Transcribe an audio file\n * ```ts\n * import { generateTranscription } from '@tanstack/ai'\n * import { openaiTranscription } from '@tanstack/ai-openai'\n *\n * const result = await generateTranscription({\n *   adapter: openaiTranscription('whisper-1'),\n *   audio: audioFile, // File, Blob, or base64 string\n *   language: 'en'\n * })\n *\n * console.log(result.text)\n * ```\n *\n * @example With verbose output for timestamps\n * ```ts\n * const result = await generateTranscription({\n *   adapter: openaiTranscription('whisper-1'),\n *   audio: audioFile,\n *   responseFormat: 'verbose_json'\n * })\n *\n * result.segments?.forEach(segment => {\n *   console.log(`[${segment.start}s - ${segment.end}s]: ${segment.text}`)\n * })\n * ```\n */\nexport async function generateTranscription<\n  TAdapter extends TranscriptionAdapter<string, object>,\n>(\n  options: TranscriptionActivityOptions<TAdapter>,\n): TranscriptionActivityResult {\n  const { adapter, ...rest } = options\n  const model = adapter.model\n  const requestId = createId('transcription')\n  const startTime = Date.now()\n\n  aiEventClient.emit('transcription:request:started', {\n    requestId,\n    provider: adapter.name,\n    model,\n    language: rest.language,\n    prompt: rest.prompt,\n    responseFormat: rest.responseFormat,\n    modelOptions: rest.modelOptions as Record<string, unknown> | undefined,\n    timestamp: startTime,\n  })\n\n  const result = await adapter.transcribe({ ...rest, model })\n  const duration = Date.now() - startTime\n\n  aiEventClient.emit('transcription:request:completed', {\n    requestId,\n    provider: adapter.name,\n    model,\n    text: result.text,\n    language: result.language,\n    duration,\n    modelOptions: rest.modelOptions as Record<string, unknown> | undefined,\n    timestamp: Date.now(),\n  })\n\n  return result\n}\n\n// ===========================\n// Options Factory\n// ===========================\n\n/**\n * Create typed options for the generateTranscription() function without executing.\n */\nexport function createTranscriptionOptions<\n  TAdapter extends TranscriptionAdapter<string, object>,\n>(\n  options: TranscriptionActivityOptions<TAdapter>,\n): TranscriptionActivityOptions<TAdapter> {\n  return options\n}\n\n// Re-export adapter types\nexport type {\n  TranscriptionAdapter,\n  TranscriptionAdapterConfig,\n  AnyTranscriptionAdapter,\n} from './adapter'\nexport { BaseTranscriptionAdapter } from './adapter'\n"],"names":[],"mappings":";AAgBO,MAAM,OAAO;AAgDpB,SAAS,SAAS,QAAwB;AACxC,SAAO,GAAG,MAAM,IAAI,KAAK,IAAA,CAAK,IAAI,KAAK,OAAA,EAAS,SAAS,EAAE,EAAE,MAAM,GAAG,CAAC,CAAC;AAC1E;AAsCA,eAAsB,sBAGpB,SAC6B;AAC7B,QAAM,EAAE,SAAS,GAAG,KAAA,IAAS;AAC7B,QAAM,QAAQ,QAAQ;AACtB,QAAM,YAAY,SAAS,eAAe;AAC1C,QAAM,YAAY,KAAK,IAAA;AAEvB,gBAAc,KAAK,iCAAiC;AAAA,IAClD;AAAA,IACA,UAAU,QAAQ;AAAA,IAClB;AAAA,IACA,UAAU,KAAK;AAAA,IACf,QAAQ,KAAK;AAAA,IACb,gBAAgB,KAAK;AAAA,IACrB,cAAc,KAAK;AAAA,IACnB,WAAW;AAAA,EAAA,CACZ;AAED,QAAM,SAAS,MAAM,QAAQ,WAAW,EAAE,GAAG,MAAM,OAAO;AAC1D,QAAM,WAAW,KAAK,IAAA,IAAQ;AAE9B,gBAAc,KAAK,mCAAmC;AAAA,IACpD;AAAA,IACA,UAAU,QAAQ;AAAA,IAClB;AAAA,IACA,MAAM,OAAO;AAAA,IACb,UAAU,OAAO;AAAA,IACjB;AAAA,IACA,cAAc,KAAK;AAAA,IACnB,WAAW,KAAK,IAAA;AAAA,EAAI,CACrB;AAED,SAAO;AACT;AASO,SAAS,2BAGd,SACwC;AACxC,SAAO;AACT;"}