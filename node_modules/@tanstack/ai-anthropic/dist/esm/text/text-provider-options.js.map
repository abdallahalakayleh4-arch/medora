{"version":3,"file":"text-provider-options.js","sources":["../../../src/text/text-provider-options.ts"],"sourcesContent":["import type {\n  BetaContextManagementConfig,\n  BetaToolChoiceAny,\n  BetaToolChoiceAuto,\n  BetaToolChoiceTool,\n} from '@anthropic-ai/sdk/resources/beta/messages/messages'\nimport type { AnthropicTool } from '../tools'\nimport type {\n  MessageParam,\n  TextBlockParam,\n} from '@anthropic-ai/sdk/resources/messages'\n\nexport interface AnthropicContainerOptions {\n  /**\n   * Container identifier for reuse across requests.\n   * Container parameters with skills to be loaded.\n   */\n  container?: {\n    id: string | null\n    /**\n     * List of skills to load into the container\n     */\n    skills: Array<{\n      /**\n       * Between 1-64 characters\n       */\n      skill_id: string\n\n      type: 'anthropic' | 'custom'\n      /**\n       * Skill version or latest by default\n       */\n      version?: string\n    }> | null\n  } | null\n}\n\nexport interface AnthropicContextManagementOptions {\n  /**\n   * Context management configuration.\n\nThis allows you to control how Claude manages context across multiple requests, such as whether to clear function results or not.\n   */\n  context_management?: BetaContextManagementConfig | null\n}\n\nexport interface AnthropicMCPOptions {\n  /**\n   * MCP servers to be utilized in this request\n   * Maximum of 20 servers\n   */\n  mcp_servers?: Array<MCPServer>\n}\n\nexport interface AnthropicServiceTierOptions {\n  /**\n   * Determines whether to use priority capacity (if available) or standard capacity for this request.\n   */\n  service_tier?: 'auto' | 'standard_only'\n}\n\nexport interface AnthropicStopSequencesOptions {\n  /**\n   * Custom text sequences that will cause the model to stop generating.\n\nAnthropic models will normally stop when they have naturally completed their turn, which will result in a response stop_reason of \"end_turn\".\n\nIf you want the model to stop generating when it encounters custom strings of text, you can use the stop_sequences parameter. If the model encounters one of the custom sequences, the response stop_reason value will be \"stop_sequence\" and the response stop_sequence value will contain the matched stop sequence.\n   */\n  stop_sequences?: Array<string>\n}\n\nexport interface AnthropicThinkingOptions {\n  /**\n     * Configuration for enabling Claude's extended thinking.\n\nWhen enabled, responses include thinking content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your max_tokens limit.\n     */\n  thinking?:\n    | {\n        /**\n* Determines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.\n\nMust be â‰¥1024 and less than max_tokens\n*/\n        budget_tokens: number\n\n        type: 'enabled'\n      }\n    | {\n        type: 'disabled'\n      }\n}\n\nexport interface AnthropicAdaptiveThinkingOptions {\n  /**\n   * Configuration for Claude's adaptive thinking (Opus 4.6+).\n   *\n   * In adaptive mode, Claude dynamically decides when and how much to think.\n   * Use the effort parameter to control thinking depth.\n   * `thinking: {type: \"enabled\"}` with `budget_tokens` is deprecated on Opus 4.6.\n   */\n  thinking?:\n    | {\n        type: 'adaptive'\n      }\n    | {\n        /**\n         * @deprecated Use `type: 'adaptive'` with the effort parameter on Opus 4.6+.\n         */\n        budget_tokens: number\n        type: 'enabled'\n      }\n    | {\n        type: 'disabled'\n      }\n}\n\nexport interface AnthropicEffortOptions {\n  /**\n   * Controls the thinking depth for adaptive thinking mode (Opus 4.6+).\n   *\n   * - `max`: Absolute highest capability\n   * - `high`: Default - Claude will almost always think\n   * - `medium`: Balanced cost-quality\n   * - `low`: May skip thinking for simpler problems\n   */\n  effort?: 'max' | 'high' | 'medium' | 'low'\n}\n\nexport interface AnthropicToolChoiceOptions {\n  tool_choice?: BetaToolChoiceAny | BetaToolChoiceTool | BetaToolChoiceAuto\n}\n\nexport interface AnthropicSamplingOptions {\n  /**\n   * Only sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses.\nRecommended for advanced use cases only. You usually only need to use temperature.\n\nRequired range: x >= 0\n   */\n  top_k?: number\n}\n\nexport type ExternalTextProviderOptions = AnthropicContainerOptions &\n  AnthropicContextManagementOptions &\n  AnthropicMCPOptions &\n  AnthropicServiceTierOptions &\n  AnthropicStopSequencesOptions &\n  AnthropicThinkingOptions &\n  AnthropicToolChoiceOptions &\n  AnthropicSamplingOptions &\n  Partial<AnthropicAdaptiveThinkingOptions> &\n  Partial<AnthropicEffortOptions>\n\nexport interface InternalTextProviderOptions extends ExternalTextProviderOptions {\n  model: string\n\n  messages: Array<MessageParam>\n\n  /**\n   * The maximum number of tokens to generate before stopping.  This parameter only specifies the absolute maximum number of tokens to generate.\n   * Range x >= 1.\n   */\n  max_tokens: number\n  /**\n   * Whether to incrementally stream the response using server-sent events.\n   */\n  stream?: boolean\n  /**\n    * stem prompt.\n \n A system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role.\n    */\n  system?: string | Array<TextBlockParam>\n  /**\n   * Amount of randomness injected into the response.\n   * Either use this or top_p, but not both.\n   * Defaults to 1.0. Ranges from 0.0 to 1.0. Use temperature closer to 0.0 for analytical / multiple choice, and closer to 1.0 for creative and generative tasks.\n   * @default 1.0\n   */\n  temperature?: number\n\n  tools?: Array<AnthropicTool>\n\n  /**\n   * Use nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by top_p. You should either alter temperature or top_p, but not both.\n   */\n  top_p?: number\n}\n\nconst validateTopPandTemperature = (options: InternalTextProviderOptions) => {\n  if (options.top_p !== undefined && options.temperature !== undefined) {\n    throw new Error('You should either set top_p or temperature, but not both.')\n  }\n}\n\nexport interface CacheControl {\n  type: 'ephemeral'\n  ttl: '5m' | '1h'\n}\n\nconst validateThinking = (options: InternalTextProviderOptions) => {\n  const thinking = options.thinking\n  if (thinking && thinking.type === 'enabled') {\n    if (thinking.budget_tokens < 1024) {\n      throw new Error('thinking.budget_tokens must be at least 1024.')\n    }\n    if (thinking.budget_tokens >= options.max_tokens) {\n      throw new Error('thinking.budget_tokens must be less than max_tokens.')\n    }\n  }\n}\n\ninterface MCPServer {\n  name: string\n  url: string\n  type: 'url'\n  authorization_token?: string | null\n  tool_configuration: {\n    allowed_tools?: Array<string> | null\n    enabled?: boolean | null\n  } | null\n}\n\nconst validateMaxTokens = (options: InternalTextProviderOptions) => {\n  if (options.max_tokens < 1) {\n    throw new Error('max_tokens must be at least 1.')\n  }\n}\n\nexport const validateTextProviderOptions = (\n  options: InternalTextProviderOptions,\n) => {\n  validateTopPandTemperature(options)\n  validateThinking(options)\n  validateMaxTokens(options)\n}\n"],"names":[],"mappings":"AAmMA,MAAM,6BAA6B,CAAC,YAAyC;AAC3E,MAAI,QAAQ,UAAU,UAAa,QAAQ,gBAAgB,QAAW;AACpE,UAAM,IAAI,MAAM,2DAA2D;AAAA,EAC7E;AACF;AAOA,MAAM,mBAAmB,CAAC,YAAyC;AACjE,QAAM,WAAW,QAAQ;AACzB,MAAI,YAAY,SAAS,SAAS,WAAW;AAC3C,QAAI,SAAS,gBAAgB,MAAM;AACjC,YAAM,IAAI,MAAM,+CAA+C;AAAA,IACjE;AACA,QAAI,SAAS,iBAAiB,QAAQ,YAAY;AAChD,YAAM,IAAI,MAAM,sDAAsD;AAAA,IACxE;AAAA,EACF;AACF;AAaA,MAAM,oBAAoB,CAAC,YAAyC;AAClE,MAAI,QAAQ,aAAa,GAAG;AAC1B,UAAM,IAAI,MAAM,gCAAgC;AAAA,EAClD;AACF;AAEO,MAAM,8BAA8B,CACzC,YACG;AACH,6BAA2B,OAAO;AAClC,mBAAiB,OAAO;AACxB,oBAAkB,OAAO;AAC3B;"}