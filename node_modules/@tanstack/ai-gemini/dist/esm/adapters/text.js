import { FinishReason } from "@google/genai";
import { BaseTextAdapter } from "@tanstack/ai/adapters";
import { convertToolsToProviderFormat } from "../tools/tool-converter.js";
import { createGeminiClient, generateId, getGeminiApiKeyFromEnv } from "../utils/client.js";
class GeminiTextAdapter extends BaseTextAdapter {
  constructor(config, model) {
    super({}, model);
    this.kind = "text";
    this.name = "gemini";
    this.client = createGeminiClient(config);
  }
  async *chatStream(options) {
    const mappedOptions = this.mapCommonOptionsToGemini(options);
    try {
      const result = await this.client.models.generateContentStream(mappedOptions);
      yield* this.processStreamChunks(result, options.model);
    } catch (error) {
      const timestamp = Date.now();
      yield {
        type: "RUN_ERROR",
        model: options.model,
        timestamp,
        error: {
          message: error instanceof Error ? error.message : "An unknown error occurred during the chat stream."
        }
      };
    }
  }
  /**
   * Generate structured output using Gemini's native JSON response format.
   * Uses responseMimeType: 'application/json' and responseSchema for structured output.
   * The outputSchema is already JSON Schema (converted in the ai layer).
   */
  async structuredOutput(options) {
    const { chatOptions, outputSchema } = options;
    const mappedOptions = this.mapCommonOptionsToGemini(chatOptions);
    try {
      const result = await this.client.models.generateContent({
        ...mappedOptions,
        config: {
          ...mappedOptions.config,
          responseMimeType: "application/json",
          responseSchema: outputSchema
        }
      });
      const rawText = this.extractTextFromResponse(result);
      let parsed;
      try {
        parsed = JSON.parse(rawText);
      } catch {
        throw new Error(
          `Failed to parse structured output as JSON. Content: ${rawText.slice(0, 200)}${rawText.length > 200 ? "..." : ""}`
        );
      }
      return {
        data: parsed,
        rawText
      };
    } catch (error) {
      throw new Error(
        error instanceof Error ? error.message : "An unknown error occurred during structured output generation."
      );
    }
  }
  /**
   * Extract text content from a non-streaming response
   */
  extractTextFromResponse(response) {
    let textContent = "";
    if (response.candidates?.[0]?.content?.parts) {
      for (const part of response.candidates[0].content.parts) {
        if (part.text) {
          textContent += part.text;
        }
      }
    }
    return textContent;
  }
  async *processStreamChunks(result, model) {
    const timestamp = Date.now();
    let accumulatedContent = "";
    let accumulatedThinking = "";
    const toolCallMap = /* @__PURE__ */ new Map();
    let nextToolIndex = 0;
    const runId = generateId(this.name);
    const messageId = generateId(this.name);
    let stepId = null;
    let hasEmittedRunStarted = false;
    let hasEmittedTextMessageStart = false;
    let hasEmittedStepStarted = false;
    for await (const chunk of result) {
      if (!hasEmittedRunStarted) {
        hasEmittedRunStarted = true;
        yield {
          type: "RUN_STARTED",
          runId,
          model,
          timestamp
        };
      }
      if (chunk.candidates?.[0]?.content?.parts) {
        const parts = chunk.candidates[0].content.parts;
        for (const part of parts) {
          if (part.text) {
            if (part.thought) {
              if (!hasEmittedStepStarted) {
                hasEmittedStepStarted = true;
                stepId = generateId(this.name);
                yield {
                  type: "STEP_STARTED",
                  stepId,
                  model,
                  timestamp,
                  stepType: "thinking"
                };
              }
              accumulatedThinking += part.text;
              yield {
                type: "STEP_FINISHED",
                stepId: stepId || generateId(this.name),
                model,
                timestamp,
                delta: part.text,
                content: accumulatedThinking
              };
            } else if (part.text.trim()) {
              if (!hasEmittedTextMessageStart) {
                hasEmittedTextMessageStart = true;
                yield {
                  type: "TEXT_MESSAGE_START",
                  messageId,
                  model,
                  timestamp,
                  role: "assistant"
                };
              }
              accumulatedContent += part.text;
              yield {
                type: "TEXT_MESSAGE_CONTENT",
                messageId,
                model,
                timestamp,
                delta: part.text,
                content: accumulatedContent
              };
            }
          }
          const functionCall = part.functionCall;
          if (functionCall) {
            const toolCallId = functionCall.id || `${functionCall.name}_${Date.now()}_${nextToolIndex}`;
            const functionArgs = functionCall.args || {};
            let toolCallData = toolCallMap.get(toolCallId);
            if (!toolCallData) {
              toolCallData = {
                name: functionCall.name || "",
                args: typeof functionArgs === "string" ? functionArgs : JSON.stringify(functionArgs),
                index: nextToolIndex++,
                started: false
              };
              toolCallMap.set(toolCallId, toolCallData);
            } else {
              try {
                const existingArgs = JSON.parse(toolCallData.args);
                const newArgs = typeof functionArgs === "string" ? JSON.parse(functionArgs) : functionArgs;
                const mergedArgs = { ...existingArgs, ...newArgs };
                toolCallData.args = JSON.stringify(mergedArgs);
              } catch {
                toolCallData.args = typeof functionArgs === "string" ? functionArgs : JSON.stringify(functionArgs);
              }
            }
            if (!toolCallData.started) {
              toolCallData.started = true;
              yield {
                type: "TOOL_CALL_START",
                toolCallId,
                toolName: toolCallData.name,
                model,
                timestamp,
                index: toolCallData.index
              };
            }
            yield {
              type: "TOOL_CALL_ARGS",
              toolCallId,
              model,
              timestamp,
              delta: toolCallData.args,
              args: toolCallData.args
            };
          }
        }
      } else if (chunk.data && chunk.data.trim()) {
        if (!hasEmittedTextMessageStart) {
          hasEmittedTextMessageStart = true;
          yield {
            type: "TEXT_MESSAGE_START",
            messageId,
            model,
            timestamp,
            role: "assistant"
          };
        }
        accumulatedContent += chunk.data;
        yield {
          type: "TEXT_MESSAGE_CONTENT",
          messageId,
          model,
          timestamp,
          delta: chunk.data,
          content: accumulatedContent
        };
      }
      if (chunk.candidates?.[0]?.finishReason) {
        const finishReason = chunk.candidates[0].finishReason;
        if (finishReason === FinishReason.UNEXPECTED_TOOL_CALL) {
          if (chunk.candidates[0].content?.parts) {
            for (const part of chunk.candidates[0].content.parts) {
              const functionCall = part.functionCall;
              if (functionCall) {
                const toolCallId = functionCall.id || `${functionCall.name}_${Date.now()}_${nextToolIndex}`;
                const functionArgs = functionCall.args || {};
                const argsString = typeof functionArgs === "string" ? functionArgs : JSON.stringify(functionArgs);
                toolCallMap.set(toolCallId, {
                  name: functionCall.name || "",
                  args: argsString,
                  index: nextToolIndex++,
                  started: true
                });
                yield {
                  type: "TOOL_CALL_START",
                  toolCallId,
                  toolName: functionCall.name || "",
                  model,
                  timestamp,
                  index: nextToolIndex - 1
                };
                let parsedInput = {};
                try {
                  parsedInput = typeof functionArgs === "string" ? JSON.parse(functionArgs) : functionArgs;
                } catch {
                  parsedInput = {};
                }
                yield {
                  type: "TOOL_CALL_END",
                  toolCallId,
                  toolName: functionCall.name || "",
                  model,
                  timestamp,
                  input: parsedInput
                };
              }
            }
          }
        }
        for (const [toolCallId, toolCallData] of toolCallMap.entries()) {
          let parsedInput = {};
          try {
            parsedInput = JSON.parse(toolCallData.args);
          } catch {
            parsedInput = {};
          }
          yield {
            type: "TOOL_CALL_END",
            toolCallId,
            toolName: toolCallData.name,
            model,
            timestamp,
            input: parsedInput
          };
        }
        if (toolCallMap.size > 0) {
          hasEmittedTextMessageStart = false;
        }
        if (finishReason === FinishReason.MAX_TOKENS) {
          yield {
            type: "RUN_ERROR",
            runId,
            model,
            timestamp,
            error: {
              message: "The response was cut off because the maximum token limit was reached.",
              code: "max_tokens"
            }
          };
        }
        if (hasEmittedTextMessageStart) {
          yield {
            type: "TEXT_MESSAGE_END",
            messageId,
            model,
            timestamp
          };
        }
        yield {
          type: "RUN_FINISHED",
          runId,
          model,
          timestamp,
          finishReason: toolCallMap.size > 0 ? "tool_calls" : "stop",
          usage: chunk.usageMetadata ? {
            promptTokens: chunk.usageMetadata.promptTokenCount ?? 0,
            completionTokens: chunk.usageMetadata.candidatesTokenCount ?? 0,
            totalTokens: chunk.usageMetadata.totalTokenCount ?? 0
          } : void 0
        };
      }
    }
  }
  convertContentPartToGemini(part) {
    switch (part.type) {
      case "text":
        return { text: part.content };
      case "image":
      case "audio":
      case "video":
      case "document": {
        if (part.source.type === "data") {
          return {
            inlineData: {
              data: part.source.value,
              mimeType: part.source.mimeType
            }
          };
        } else {
          const defaultMimeType = {
            image: "image/jpeg",
            audio: "audio/mp3",
            video: "video/mp4",
            document: "application/pdf"
          }[part.type];
          return {
            fileData: {
              fileUri: part.source.value,
              mimeType: part.source.mimeType ?? defaultMimeType
            }
          };
        }
      }
      default: {
        const _exhaustiveCheck = part;
        throw new Error(
          `Unsupported content part type: ${_exhaustiveCheck.type}`
        );
      }
    }
  }
  formatMessages(messages) {
    const formatted = messages.map((msg) => {
      const role = msg.role === "assistant" ? "model" : "user";
      const parts = [];
      if (Array.isArray(msg.content)) {
        for (const contentPart of msg.content) {
          parts.push(this.convertContentPartToGemini(contentPart));
        }
      } else if (msg.content) {
        parts.push({ text: msg.content });
      }
      if (msg.role === "assistant" && msg.toolCalls?.length) {
        for (const toolCall of msg.toolCalls) {
          let parsedArgs = {};
          try {
            parsedArgs = toolCall.function.arguments ? JSON.parse(toolCall.function.arguments) : {};
          } catch {
            parsedArgs = toolCall.function.arguments;
          }
          parts.push({
            functionCall: {
              name: toolCall.function.name,
              args: parsedArgs
            }
          });
        }
      }
      if (msg.role === "tool" && msg.toolCallId) {
        parts.push({
          functionResponse: {
            name: msg.toolCallId,
            response: {
              content: msg.content || ""
            }
          }
        });
      }
      return {
        role,
        parts: parts.length > 0 ? parts : [{ text: "" }]
      };
    });
    return this.mergeConsecutiveSameRoleMessages(formatted);
  }
  /**
   * Merge consecutive messages of the same role into a single message.
   * Gemini's API requires strictly alternating user/model roles.
   * Tool results are mapped to role:'user', which can collide with actual
   * user messages in multi-turn conversations.
   *
   * Also filters out empty model messages (e.g., from a previous failed request)
   * and deduplicates functionResponse parts with the same name (tool call ID).
   */
  mergeConsecutiveSameRoleMessages(messages) {
    const merged = [];
    for (const msg of messages) {
      const parts = msg.parts || [];
      if (msg.role === "model") {
        const hasContent = parts.length > 0 && !parts.every(
          (p) => "text" in p && p.text === ""
        );
        if (!hasContent) {
          continue;
        }
      }
      const prev = merged[merged.length - 1];
      if (prev && prev.role === msg.role) {
        prev.parts = [...prev.parts || [], ...parts];
      } else {
        merged.push({ ...msg, parts: [...parts] });
      }
    }
    for (const msg of merged) {
      if (!msg.parts) continue;
      const seenFunctionResponseNames = /* @__PURE__ */ new Set();
      msg.parts = msg.parts.filter((part) => {
        if ("functionResponse" in part && part.functionResponse?.name) {
          if (seenFunctionResponseNames.has(part.functionResponse.name)) {
            return false;
          }
          seenFunctionResponseNames.add(part.functionResponse.name);
        }
        return true;
      });
    }
    return merged;
  }
  mapCommonOptionsToGemini(options) {
    const modelOpts = options.modelOptions;
    const thinkingConfig = modelOpts?.thinkingConfig;
    const requestOptions = {
      model: options.model,
      contents: this.formatMessages(options.messages),
      config: {
        ...modelOpts,
        temperature: options.temperature,
        topP: options.topP,
        maxOutputTokens: options.maxTokens,
        thinkingConfig: thinkingConfig ? {
          ...thinkingConfig,
          thinkingLevel: thinkingConfig.thinkingLevel ? (
            // Enum is provided by the SDK, we use it for the type but cast it to string constants, here we just cast them back
            thinkingConfig.thinkingLevel
          ) : void 0
        } : void 0,
        systemInstruction: options.systemPrompts?.join("\n"),
        tools: convertToolsToProviderFormat(options.tools)
      }
    };
    return requestOptions;
  }
}
function createGeminiChat(model, apiKey, config) {
  return new GeminiTextAdapter({ apiKey, ...config }, model);
}
function geminiText(model, config) {
  const apiKey = getGeminiApiKeyFromEnv();
  return createGeminiChat(model, apiKey, config);
}
export {
  GeminiTextAdapter,
  createGeminiChat,
  geminiText
};
//# sourceMappingURL=text.js.map
