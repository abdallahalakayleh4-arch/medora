import { BaseTextAdapter, StructuredOutputOptions, StructuredOutputResult } from '@tanstack/ai/adapters';
import { OLLAMA_TEXT_MODELS, OllamaChatModelOptionsByName } from '../model-meta.js';
import { ChatRequest, Ollama } from 'ollama';
import { StreamChunk, TextOptions } from '@tanstack/ai';
export type OllamaTextModel = (typeof OLLAMA_TEXT_MODELS)[number] | (string & {});
/**
 * Resolve model options for a specific model.
 * If the model has explicit options in the map, use those; otherwise use base options.
 */
type ResolveModelOptions<TModel extends string> = TModel extends keyof OllamaChatModelOptionsByName ? OllamaChatModelOptionsByName[TModel] : ChatRequest;
/**
 * Ollama-specific provider options
 */
export interface OllamaTextProviderOptions {
    /** Number of tokens to keep from the prompt */
    num_keep?: number;
    /** Number of tokens from context to consider for next token prediction */
    top_k?: number;
    /** Minimum probability for nucleus sampling */
    min_p?: number;
    /** Tail-free sampling parameter */
    tfs_z?: number;
    /** Typical probability sampling parameter */
    typical_p?: number;
    /** Number of previous tokens to consider for repetition penalty */
    repeat_last_n?: number;
    /** Penalty for repeating tokens */
    repeat_penalty?: number;
    /** Enable Mirostat sampling (0=disabled, 1=Mirostat, 2=Mirostat 2.0) */
    mirostat?: number;
    /** Target entropy for Mirostat */
    mirostat_tau?: number;
    /** Learning rate for Mirostat */
    mirostat_eta?: number;
    /** Enable penalize_newline */
    penalize_newline?: boolean;
    /** Enable NUMA support */
    numa?: boolean;
    /** Context window size */
    num_ctx?: number;
    /** Batch size for prompt processing */
    num_batch?: number;
    /** Number of GQA groups (for some models) */
    num_gqa?: number;
    /** Number of GPU layers to use */
    num_gpu?: number;
    /** GPU to use for inference */
    main_gpu?: number;
    /** Use memory-mapped model */
    use_mmap?: boolean;
    /** Use memory-locked model */
    use_mlock?: boolean;
    /** Number of threads to use */
    num_thread?: number;
}
export interface OllamaTextAdapterOptions {
    model?: OllamaTextModel;
    host?: string;
}
/**
 * Default input modalities for Ollama models
 */
type OllamaInputModalities = readonly ['text', 'image'];
/**
 * Default message metadata for Ollama
 */
type OllamaMessageMetadataByModality = {
    text: unknown;
    image: unknown;
    audio: unknown;
    video: unknown;
    document: unknown;
};
/**
 * Ollama Text/Chat Adapter
 * A tree-shakeable chat adapter for Ollama
 *
 * Note: Ollama supports any model name as a string since models are loaded dynamically.
 * The predefined OllamaTextModels are common models but any string is accepted.
 */
export declare class OllamaTextAdapter<TModel extends string> extends BaseTextAdapter<TModel, ResolveModelOptions<TModel>, OllamaInputModalities, OllamaMessageMetadataByModality> {
    readonly kind: "text";
    readonly name: "ollama";
    private client;
    constructor(hostOrClient: string | Ollama | undefined, model: TModel);
    chatStream(options: TextOptions): AsyncIterable<StreamChunk>;
    /**
     * Generate structured output using Ollama's JSON format option.
     * Uses format: 'json' with the schema to ensure structured output.
     * The outputSchema is already JSON Schema (converted in the ai layer).
     */
    structuredOutput(options: StructuredOutputOptions<ResolveModelOptions<TModel>>): Promise<StructuredOutputResult<unknown>>;
    private processOllamaStreamChunks;
    private convertToolsToOllamaFormat;
    private formatMessages;
    private mapCommonOptionsToOllama;
}
/**
 * Creates an Ollama chat adapter with explicit host.
 * Type resolution happens here at the call site.
 */
export declare function createOllamaChat<TModel extends string>(model: TModel, host?: string): OllamaTextAdapter<TModel>;
/**
 * Creates an Ollama text adapter with host from environment.
 * Type resolution happens here at the call site.
 */
export declare function ollamaText<TModel extends string>(model: TModel): OllamaTextAdapter<TModel>;
export {};
