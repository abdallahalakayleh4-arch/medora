import { BaseTextAdapter } from "@tanstack/ai/adapters";
import { createOllamaClient, generateId, getOllamaHostFromEnv } from "../utils/client.js";
class OllamaTextAdapter extends BaseTextAdapter {
  constructor(hostOrClient, model) {
    super({}, model);
    this.kind = "text";
    this.name = "ollama";
    if (typeof hostOrClient === "string" || hostOrClient === void 0) {
      this.client = createOllamaClient({ host: hostOrClient });
    } else {
      this.client = hostOrClient;
    }
  }
  async *chatStream(options) {
    const mappedOptions = this.mapCommonOptionsToOllama(options);
    const response = await this.client.chat({
      ...mappedOptions,
      stream: true
    });
    yield* this.processOllamaStreamChunks(response);
  }
  /**
   * Generate structured output using Ollama's JSON format option.
   * Uses format: 'json' with the schema to ensure structured output.
   * The outputSchema is already JSON Schema (converted in the ai layer).
   */
  async structuredOutput(options) {
    const { chatOptions, outputSchema } = options;
    const mappedOptions = this.mapCommonOptionsToOllama(chatOptions);
    try {
      const response = await this.client.chat({
        ...mappedOptions,
        stream: false,
        format: outputSchema
      });
      const rawText = response.message.content;
      let parsed;
      try {
        parsed = JSON.parse(rawText);
      } catch {
        throw new Error(
          `Failed to parse structured output as JSON. Content: ${rawText.slice(0, 200)}${rawText.length > 200 ? "..." : ""}`
        );
      }
      return {
        data: parsed,
        rawText
      };
    } catch (error) {
      const err = error;
      throw new Error(
        `Structured output generation failed: ${err.message || "Unknown error occurred"}`
      );
    }
  }
  async *processOllamaStreamChunks(stream) {
    let accumulatedContent = "";
    const timestamp = Date.now();
    let accumulatedReasoning = "";
    const toolCallsEmitted = /* @__PURE__ */ new Set();
    const runId = generateId("run");
    const messageId = generateId("msg");
    let stepId = null;
    let hasEmittedRunStarted = false;
    let hasEmittedTextMessageStart = false;
    let hasEmittedStepStarted = false;
    for await (const chunk of stream) {
      if (!hasEmittedRunStarted) {
        hasEmittedRunStarted = true;
        yield {
          type: "RUN_STARTED",
          runId,
          model: chunk.model,
          timestamp
        };
      }
      const handleToolCall = (toolCall) => {
        const actualToolCall = toolCall;
        const toolCallId = actualToolCall.id || `${actualToolCall.function.name}_${Date.now()}`;
        const events = [];
        if (!toolCallsEmitted.has(toolCallId)) {
          toolCallsEmitted.add(toolCallId);
          events.push({
            type: "TOOL_CALL_START",
            toolCallId,
            toolName: actualToolCall.function.name || "",
            model: chunk.model,
            timestamp,
            index: actualToolCall.function.index
          });
        }
        let parsedInput = {};
        const argsStr = typeof actualToolCall.function.arguments === "string" ? actualToolCall.function.arguments : JSON.stringify(actualToolCall.function.arguments);
        try {
          parsedInput = JSON.parse(argsStr);
        } catch {
          parsedInput = actualToolCall.function.arguments;
        }
        events.push({
          type: "TOOL_CALL_END",
          toolCallId,
          toolName: actualToolCall.function.name || "",
          model: chunk.model,
          timestamp,
          input: parsedInput
        });
        return events;
      };
      if (chunk.done) {
        if (chunk.message.tool_calls && chunk.message.tool_calls.length > 0) {
          for (const toolCall of chunk.message.tool_calls) {
            const events = handleToolCall(toolCall);
            for (const event of events) {
              yield event;
            }
          }
        }
        if (hasEmittedTextMessageStart) {
          yield {
            type: "TEXT_MESSAGE_END",
            messageId,
            model: chunk.model,
            timestamp
          };
        }
        yield {
          type: "RUN_FINISHED",
          runId,
          model: chunk.model,
          timestamp,
          finishReason: toolCallsEmitted.size > 0 ? "tool_calls" : "stop"
        };
        continue;
      }
      if (chunk.message.content) {
        if (!hasEmittedTextMessageStart) {
          hasEmittedTextMessageStart = true;
          yield {
            type: "TEXT_MESSAGE_START",
            messageId,
            model: chunk.model,
            timestamp,
            role: "assistant"
          };
        }
        accumulatedContent += chunk.message.content;
        yield {
          type: "TEXT_MESSAGE_CONTENT",
          messageId,
          model: chunk.model,
          timestamp,
          delta: chunk.message.content,
          content: accumulatedContent
        };
      }
      if (chunk.message.tool_calls && chunk.message.tool_calls.length > 0) {
        for (const toolCall of chunk.message.tool_calls) {
          const events = handleToolCall(toolCall);
          for (const event of events) {
            yield event;
          }
        }
      }
      if (chunk.message.thinking) {
        if (!hasEmittedStepStarted) {
          hasEmittedStepStarted = true;
          stepId = generateId("step");
          yield {
            type: "STEP_STARTED",
            stepId,
            model: chunk.model,
            timestamp,
            stepType: "thinking"
          };
        }
        accumulatedReasoning += chunk.message.thinking;
        yield {
          type: "STEP_FINISHED",
          stepId: stepId || generateId("step"),
          model: chunk.model,
          timestamp,
          delta: chunk.message.thinking,
          content: accumulatedReasoning
        };
      }
    }
  }
  convertToolsToOllamaFormat(tools) {
    if (!tools || tools.length === 0) {
      return void 0;
    }
    return tools.map((tool) => ({
      type: "function",
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.inputSchema ?? {
          type: "object",
          properties: {},
          required: []
        }
      }
    }));
  }
  formatMessages(messages) {
    return messages.map((msg) => {
      let textContent = "";
      const images = [];
      if (Array.isArray(msg.content)) {
        for (const part of msg.content) {
          if (part.type === "text") {
            textContent += part.content;
          } else if (part.type === "image") {
            if (part.source.type === "data") {
              images.push(part.source.value);
            } else {
              images.push(part.source.value);
            }
          }
        }
      } else {
        textContent = msg.content || "";
      }
      const hasToolCallId = msg.role === "tool" && msg.toolCallId;
      return {
        role: hasToolCallId ? "tool" : msg.role,
        content: hasToolCallId ? typeof msg.content === "string" ? msg.content : JSON.stringify(msg.content) : textContent,
        ...images.length > 0 ? { images } : {},
        ...msg.role === "assistant" && msg.toolCalls && msg.toolCalls.length > 0 ? {
          tool_calls: msg.toolCalls.map((toolCall) => {
            let parsedArguments = {};
            if (typeof toolCall.function.arguments === "string") {
              try {
                parsedArguments = JSON.parse(
                  toolCall.function.arguments
                );
              } catch {
                parsedArguments = {};
              }
            } else {
              parsedArguments = toolCall.function.arguments;
            }
            return {
              id: toolCall.id,
              type: toolCall.type,
              function: {
                name: toolCall.function.name,
                arguments: parsedArguments
              }
            };
          })
        } : {}
      };
    });
  }
  mapCommonOptionsToOllama(options) {
    const model = options.model;
    const modelOptions = options.modelOptions;
    const ollamaOptions = {
      temperature: options.temperature,
      top_p: options.topP,
      num_predict: options.maxTokens,
      ...modelOptions
    };
    return {
      model,
      options: ollamaOptions,
      messages: this.formatMessages(options.messages),
      tools: this.convertToolsToOllamaFormat(options.tools)
    };
  }
}
function createOllamaChat(model, host) {
  return new OllamaTextAdapter(host, model);
}
function ollamaText(model) {
  const host = getOllamaHostFromEnv();
  return new OllamaTextAdapter(host, model);
}
export {
  OllamaTextAdapter,
  createOllamaChat,
  ollamaText
};
//# sourceMappingURL=text.js.map
