{"version":3,"file":"summarize.js","sources":["../../../src/adapters/summarize.ts"],"sourcesContent":["import {\n  createOllamaClient,\n  estimateTokens,\n  generateId,\n  getOllamaHostFromEnv,\n} from '../utils'\n\nimport type { OLLAMA_TEXT_MODELS as OllamaSummarizeModels } from '../model-meta'\nimport type { Ollama } from 'ollama'\nimport type { SummarizeAdapter } from '@tanstack/ai/adapters'\nimport type {\n  StreamChunk,\n  SummarizationOptions,\n  SummarizationResult,\n} from '@tanstack/ai'\n\nexport type OllamaSummarizeModel =\n  | (typeof OllamaSummarizeModels)[number]\n  | (string & {})\n\n/**\n * Ollama-specific provider options for summarization\n */\nexport interface OllamaSummarizeProviderOptions {\n  /** Number of GPU layers to use */\n  num_gpu?: number\n  /** Number of threads to use */\n  num_thread?: number\n  /** Context window size */\n  num_ctx?: number\n  /** Number of tokens to predict */\n  num_predict?: number\n  /** Temperature for sampling */\n  temperature?: number\n  /** Top-p sampling */\n  top_p?: number\n  /** Top-k sampling */\n  top_k?: number\n  /** Repeat penalty */\n  repeat_penalty?: number\n}\n\nexport interface OllamaSummarizeAdapterOptions {\n  host?: string\n}\n\n/**\n * Ollama Summarize Adapter\n * A tree-shakeable summarization adapter for Ollama\n */\nexport class OllamaSummarizeAdapter<\n  TModel extends OllamaSummarizeModel,\n> implements SummarizeAdapter<TModel, OllamaSummarizeProviderOptions> {\n  readonly kind = 'summarize' as const\n  readonly name = 'ollama' as const\n  readonly model: TModel\n\n  // Type-only property - never assigned at runtime\n  declare '~types': {\n    providerOptions: OllamaSummarizeProviderOptions\n  }\n\n  private client: Ollama\n  constructor(\n    hostOrClient: string | Ollama | undefined,\n    model: TModel,\n    _options: OllamaSummarizeAdapterOptions = {},\n  ) {\n    if (typeof hostOrClient === 'string' || hostOrClient === undefined) {\n      this.client = createOllamaClient({ host: hostOrClient })\n    } else {\n      this.client = hostOrClient\n    }\n    this.model = model\n  }\n\n  async summarize(options: SummarizationOptions): Promise<SummarizationResult> {\n    const model = options.model\n\n    const prompt = this.buildSummarizationPrompt(options)\n\n    const response = await this.client.generate({\n      model,\n      prompt,\n      options: {\n        temperature: 0.3,\n        num_predict: options.maxLength ?? 500,\n      },\n      stream: false,\n    })\n\n    const promptTokens = estimateTokens(prompt)\n    const completionTokens = estimateTokens(response.response)\n\n    return {\n      id: generateId('sum'),\n      model: response.model,\n      summary: response.response,\n      usage: {\n        promptTokens,\n        completionTokens,\n        totalTokens: promptTokens + completionTokens,\n      },\n    }\n  }\n\n  async *summarizeStream(\n    options: SummarizationOptions,\n  ): AsyncIterable<StreamChunk> {\n    const model = options.model\n    const id = generateId('sum')\n    const prompt = this.buildSummarizationPrompt(options)\n    let accumulatedContent = ''\n\n    const stream = await this.client.generate({\n      model,\n      prompt,\n      options: {\n        temperature: 0.3,\n        num_predict: options.maxLength ?? 500,\n      },\n      stream: true,\n    })\n\n    for await (const chunk of stream) {\n      if (chunk.response) {\n        accumulatedContent += chunk.response\n        yield {\n          type: 'TEXT_MESSAGE_CONTENT',\n          messageId: id,\n          model: chunk.model,\n          timestamp: Date.now(),\n          delta: chunk.response,\n          content: accumulatedContent,\n        }\n      }\n\n      if (chunk.done) {\n        const promptTokens = estimateTokens(prompt)\n        const completionTokens = estimateTokens(accumulatedContent)\n        yield {\n          type: 'RUN_FINISHED',\n          runId: id,\n          model: chunk.model,\n          timestamp: Date.now(),\n          finishReason: 'stop',\n          usage: {\n            promptTokens,\n            completionTokens,\n            totalTokens: promptTokens + completionTokens,\n          },\n        }\n      }\n    }\n  }\n\n  private buildSummarizationPrompt(options: SummarizationOptions): string {\n    let prompt = 'You are a professional summarizer. '\n\n    switch (options.style) {\n      case 'bullet-points':\n        prompt += 'Provide a summary in bullet point format. '\n        break\n      case 'concise':\n        prompt += 'Provide a very brief one or two sentence summary. '\n        break\n      case 'paragraph':\n      default:\n        prompt += 'Provide a clear and concise summary in paragraph format. '\n    }\n\n    if (options.maxLength) {\n      prompt += `Keep the summary under ${options.maxLength} words. `\n    }\n\n    if (options.focus && options.focus.length > 0) {\n      prompt += `Focus on: ${options.focus.join(', ')}. `\n    }\n\n    prompt += `\\n\\nText to summarize:\\n${options.text}\\n\\nSummary:`\n\n    return prompt\n  }\n}\n\n/**\n * Creates an Ollama summarize adapter with explicit host and model\n */\nexport function createOllamaSummarize<TModel extends OllamaSummarizeModel>(\n  model: TModel,\n  host?: string,\n  options?: OllamaSummarizeAdapterOptions,\n): OllamaSummarizeAdapter<TModel> {\n  return new OllamaSummarizeAdapter(host, model, options)\n}\n\n/**\n * Creates an Ollama summarize adapter with host from environment and required model\n */\nexport function ollamaSummarize<TModel extends OllamaSummarizeModel>(\n  model: TModel,\n  options?: OllamaSummarizeAdapterOptions,\n): OllamaSummarizeAdapter<TModel> {\n  const host = getOllamaHostFromEnv()\n  return new OllamaSummarizeAdapter(host, model, options)\n}\n"],"names":[],"mappings":";AAkDO,MAAM,uBAEyD;AAAA,EAWpE,YACE,cACA,OACA,WAA0C,CAAA,GAC1C;AAdF,SAAS,OAAO;AAChB,SAAS,OAAO;AAcd,QAAI,OAAO,iBAAiB,YAAY,iBAAiB,QAAW;AAClE,WAAK,SAAS,mBAAmB,EAAE,MAAM,cAAc;AAAA,IACzD,OAAO;AACL,WAAK,SAAS;AAAA,IAChB;AACA,SAAK,QAAQ;AAAA,EACf;AAAA,EAEA,MAAM,UAAU,SAA6D;AAC3E,UAAM,QAAQ,QAAQ;AAEtB,UAAM,SAAS,KAAK,yBAAyB,OAAO;AAEpD,UAAM,WAAW,MAAM,KAAK,OAAO,SAAS;AAAA,MAC1C;AAAA,MACA;AAAA,MACA,SAAS;AAAA,QACP,aAAa;AAAA,QACb,aAAa,QAAQ,aAAa;AAAA,MAAA;AAAA,MAEpC,QAAQ;AAAA,IAAA,CACT;AAED,UAAM,eAAe,eAAe,MAAM;AAC1C,UAAM,mBAAmB,eAAe,SAAS,QAAQ;AAEzD,WAAO;AAAA,MACL,IAAI,WAAW,KAAK;AAAA,MACpB,OAAO,SAAS;AAAA,MAChB,SAAS,SAAS;AAAA,MAClB,OAAO;AAAA,QACL;AAAA,QACA;AAAA,QACA,aAAa,eAAe;AAAA,MAAA;AAAA,IAC9B;AAAA,EAEJ;AAAA,EAEA,OAAO,gBACL,SAC4B;AAC5B,UAAM,QAAQ,QAAQ;AACtB,UAAM,KAAK,WAAW,KAAK;AAC3B,UAAM,SAAS,KAAK,yBAAyB,OAAO;AACpD,QAAI,qBAAqB;AAEzB,UAAM,SAAS,MAAM,KAAK,OAAO,SAAS;AAAA,MACxC;AAAA,MACA;AAAA,MACA,SAAS;AAAA,QACP,aAAa;AAAA,QACb,aAAa,QAAQ,aAAa;AAAA,MAAA;AAAA,MAEpC,QAAQ;AAAA,IAAA,CACT;AAED,qBAAiB,SAAS,QAAQ;AAChC,UAAI,MAAM,UAAU;AAClB,8BAAsB,MAAM;AAC5B,cAAM;AAAA,UACJ,MAAM;AAAA,UACN,WAAW;AAAA,UACX,OAAO,MAAM;AAAA,UACb,WAAW,KAAK,IAAA;AAAA,UAChB,OAAO,MAAM;AAAA,UACb,SAAS;AAAA,QAAA;AAAA,MAEb;AAEA,UAAI,MAAM,MAAM;AACd,cAAM,eAAe,eAAe,MAAM;AAC1C,cAAM,mBAAmB,eAAe,kBAAkB;AAC1D,cAAM;AAAA,UACJ,MAAM;AAAA,UACN,OAAO;AAAA,UACP,OAAO,MAAM;AAAA,UACb,WAAW,KAAK,IAAA;AAAA,UAChB,cAAc;AAAA,UACd,OAAO;AAAA,YACL;AAAA,YACA;AAAA,YACA,aAAa,eAAe;AAAA,UAAA;AAAA,QAC9B;AAAA,MAEJ;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,yBAAyB,SAAuC;AACtE,QAAI,SAAS;AAEb,YAAQ,QAAQ,OAAA;AAAA,MACd,KAAK;AACH,kBAAU;AACV;AAAA,MACF,KAAK;AACH,kBAAU;AACV;AAAA,MACF,KAAK;AAAA,MACL;AACE,kBAAU;AAAA,IAAA;AAGd,QAAI,QAAQ,WAAW;AACrB,gBAAU,0BAA0B,QAAQ,SAAS;AAAA,IACvD;AAEA,QAAI,QAAQ,SAAS,QAAQ,MAAM,SAAS,GAAG;AAC7C,gBAAU,aAAa,QAAQ,MAAM,KAAK,IAAI,CAAC;AAAA,IACjD;AAEA,cAAU;AAAA;AAAA;AAAA,EAA2B,QAAQ,IAAI;AAAA;AAAA;AAEjD,WAAO;AAAA,EACT;AACF;AAKO,SAAS,sBACd,OACA,MACA,SACgC;AAChC,SAAO,IAAI,uBAAuB,MAAM,OAAO,OAAO;AACxD;AAKO,SAAS,gBACd,OACA,SACgC;AAChC,QAAM,OAAO,qBAAA;AACb,SAAO,IAAI,uBAAuB,MAAM,OAAO,OAAO;AACxD;"}