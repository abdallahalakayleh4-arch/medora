{"version":3,"file":"text.js","sources":["../../../src/adapters/text.ts"],"sourcesContent":["import { BaseTextAdapter } from '@tanstack/ai/adapters'\n\nimport { createOllamaClient, generateId, getOllamaHostFromEnv } from '../utils'\n\nimport type {\n  OLLAMA_TEXT_MODELS,\n  OllamaChatModelOptionsByName,\n} from '../model-meta'\nimport type {\n  StructuredOutputOptions,\n  StructuredOutputResult,\n} from '@tanstack/ai/adapters'\nimport type {\n  AbortableAsyncIterator,\n  ChatRequest,\n  ChatResponse,\n  Message,\n  Ollama,\n  Tool as OllamaTool,\n  ToolCall,\n} from 'ollama'\nimport type { StreamChunk, TextOptions, Tool } from '@tanstack/ai'\n\nexport type OllamaTextModel =\n  | (typeof OLLAMA_TEXT_MODELS)[number]\n  | (string & {})\n\n/**\n * Resolve model options for a specific model.\n * If the model has explicit options in the map, use those; otherwise use base options.\n */\ntype ResolveModelOptions<TModel extends string> =\n  TModel extends keyof OllamaChatModelOptionsByName\n    ? OllamaChatModelOptionsByName[TModel]\n    : ChatRequest\n\n/**\n * Ollama-specific provider options\n */\nexport interface OllamaTextProviderOptions {\n  /** Number of tokens to keep from the prompt */\n  num_keep?: number\n  /** Number of tokens from context to consider for next token prediction */\n  top_k?: number\n  /** Minimum probability for nucleus sampling */\n  min_p?: number\n  /** Tail-free sampling parameter */\n  tfs_z?: number\n  /** Typical probability sampling parameter */\n  typical_p?: number\n  /** Number of previous tokens to consider for repetition penalty */\n  repeat_last_n?: number\n  /** Penalty for repeating tokens */\n  repeat_penalty?: number\n  /** Enable Mirostat sampling (0=disabled, 1=Mirostat, 2=Mirostat 2.0) */\n  mirostat?: number\n  /** Target entropy for Mirostat */\n  mirostat_tau?: number\n  /** Learning rate for Mirostat */\n  mirostat_eta?: number\n  /** Enable penalize_newline */\n  penalize_newline?: boolean\n  /** Enable NUMA support */\n  numa?: boolean\n  /** Context window size */\n  num_ctx?: number\n  /** Batch size for prompt processing */\n  num_batch?: number\n  /** Number of GQA groups (for some models) */\n  num_gqa?: number\n  /** Number of GPU layers to use */\n  num_gpu?: number\n  /** GPU to use for inference */\n  main_gpu?: number\n  /** Use memory-mapped model */\n  use_mmap?: boolean\n  /** Use memory-locked model */\n  use_mlock?: boolean\n  /** Number of threads to use */\n  num_thread?: number\n}\n\nexport interface OllamaTextAdapterOptions {\n  model?: OllamaTextModel\n  host?: string\n}\n\n/**\n * Default input modalities for Ollama models\n */\ntype OllamaInputModalities = readonly ['text', 'image']\n\n/**\n * Default message metadata for Ollama\n */\ntype OllamaMessageMetadataByModality = {\n  text: unknown\n  image: unknown\n  audio: unknown\n  video: unknown\n  document: unknown\n}\n\n/**\n * Ollama Text/Chat Adapter\n * A tree-shakeable chat adapter for Ollama\n *\n * Note: Ollama supports any model name as a string since models are loaded dynamically.\n * The predefined OllamaTextModels are common models but any string is accepted.\n */\nexport class OllamaTextAdapter<TModel extends string> extends BaseTextAdapter<\n  TModel,\n  ResolveModelOptions<TModel>,\n  OllamaInputModalities,\n  OllamaMessageMetadataByModality\n> {\n  readonly kind = 'text' as const\n  readonly name = 'ollama' as const\n\n  private client: Ollama\n\n  constructor(hostOrClient: string | Ollama | undefined, model: TModel) {\n    super({}, model)\n    if (typeof hostOrClient === 'string' || hostOrClient === undefined) {\n      this.client = createOllamaClient({ host: hostOrClient })\n    } else {\n      this.client = hostOrClient\n    }\n  }\n\n  async *chatStream(options: TextOptions): AsyncIterable<StreamChunk> {\n    const mappedOptions = this.mapCommonOptionsToOllama(options)\n    const response = await this.client.chat({\n      ...mappedOptions,\n      stream: true,\n    })\n    yield* this.processOllamaStreamChunks(response)\n  }\n\n  /**\n   * Generate structured output using Ollama's JSON format option.\n   * Uses format: 'json' with the schema to ensure structured output.\n   * The outputSchema is already JSON Schema (converted in the ai layer).\n   */\n  async structuredOutput(\n    options: StructuredOutputOptions<ResolveModelOptions<TModel>>,\n  ): Promise<StructuredOutputResult<unknown>> {\n    const { chatOptions, outputSchema } = options\n\n    const mappedOptions = this.mapCommonOptionsToOllama(chatOptions)\n\n    try {\n      // Make non-streaming request with JSON format\n      const response = await this.client.chat({\n        ...mappedOptions,\n        stream: false,\n        format: outputSchema,\n      })\n\n      const rawText = response.message.content\n\n      // Parse the JSON response\n      let parsed: unknown\n      try {\n        parsed = JSON.parse(rawText)\n      } catch {\n        throw new Error(\n          `Failed to parse structured output as JSON. Content: ${rawText.slice(0, 200)}${rawText.length > 200 ? '...' : ''}`,\n        )\n      }\n\n      return {\n        data: parsed,\n        rawText,\n      }\n    } catch (error: unknown) {\n      const err = error as Error\n      throw new Error(\n        `Structured output generation failed: ${err.message || 'Unknown error occurred'}`,\n      )\n    }\n  }\n\n  private async *processOllamaStreamChunks(\n    stream: AbortableAsyncIterator<ChatResponse>,\n  ): AsyncIterable<StreamChunk> {\n    let accumulatedContent = ''\n    const timestamp = Date.now()\n    let accumulatedReasoning = ''\n    const toolCallsEmitted = new Set<string>()\n\n    // AG-UI lifecycle tracking\n    const runId = generateId('run')\n    const messageId = generateId('msg')\n    let stepId: string | null = null\n    let hasEmittedRunStarted = false\n    let hasEmittedTextMessageStart = false\n    let hasEmittedStepStarted = false\n\n    for await (const chunk of stream) {\n      // Emit RUN_STARTED on first chunk\n      if (!hasEmittedRunStarted) {\n        hasEmittedRunStarted = true\n        yield {\n          type: 'RUN_STARTED',\n          runId,\n          model: chunk.model,\n          timestamp,\n        }\n      }\n\n      const handleToolCall = (toolCall: ToolCall): Array<StreamChunk> => {\n        const actualToolCall = toolCall as ToolCall & {\n          id: string\n          function: { index: number }\n        }\n        const toolCallId =\n          actualToolCall.id || `${actualToolCall.function.name}_${Date.now()}`\n        const events: Array<StreamChunk> = []\n\n        // Emit TOOL_CALL_START if not already emitted for this tool call\n        if (!toolCallsEmitted.has(toolCallId)) {\n          toolCallsEmitted.add(toolCallId)\n          events.push({\n            type: 'TOOL_CALL_START',\n            toolCallId,\n            toolName: actualToolCall.function.name || '',\n            model: chunk.model,\n            timestamp,\n            index: actualToolCall.function.index,\n          })\n        }\n\n        // Parse input\n        let parsedInput: unknown = {}\n        const argsStr =\n          typeof actualToolCall.function.arguments === 'string'\n            ? actualToolCall.function.arguments\n            : JSON.stringify(actualToolCall.function.arguments)\n        try {\n          parsedInput = JSON.parse(argsStr)\n        } catch {\n          parsedInput = actualToolCall.function.arguments\n        }\n\n        // Emit TOOL_CALL_END\n        events.push({\n          type: 'TOOL_CALL_END',\n          toolCallId,\n          toolName: actualToolCall.function.name || '',\n          model: chunk.model,\n          timestamp,\n          input: parsedInput,\n        })\n\n        return events\n      }\n\n      if (chunk.done) {\n        if (chunk.message.tool_calls && chunk.message.tool_calls.length > 0) {\n          for (const toolCall of chunk.message.tool_calls) {\n            const events = handleToolCall(toolCall)\n            for (const event of events) {\n              yield event\n            }\n          }\n        }\n\n        // Emit TEXT_MESSAGE_END if we had text content\n        if (hasEmittedTextMessageStart) {\n          yield {\n            type: 'TEXT_MESSAGE_END',\n            messageId,\n            model: chunk.model,\n            timestamp,\n          }\n        }\n\n        yield {\n          type: 'RUN_FINISHED',\n          runId,\n          model: chunk.model,\n          timestamp,\n          finishReason: toolCallsEmitted.size > 0 ? 'tool_calls' : 'stop',\n        }\n        continue\n      }\n\n      if (chunk.message.content) {\n        // Emit TEXT_MESSAGE_START on first text content\n        if (!hasEmittedTextMessageStart) {\n          hasEmittedTextMessageStart = true\n          yield {\n            type: 'TEXT_MESSAGE_START',\n            messageId,\n            model: chunk.model,\n            timestamp,\n            role: 'assistant',\n          }\n        }\n\n        accumulatedContent += chunk.message.content\n        yield {\n          type: 'TEXT_MESSAGE_CONTENT',\n          messageId,\n          model: chunk.model,\n          timestamp,\n          delta: chunk.message.content,\n          content: accumulatedContent,\n        }\n      }\n\n      if (chunk.message.tool_calls && chunk.message.tool_calls.length > 0) {\n        for (const toolCall of chunk.message.tool_calls) {\n          const events = handleToolCall(toolCall)\n          for (const event of events) {\n            yield event\n          }\n        }\n      }\n\n      if (chunk.message.thinking) {\n        // Emit STEP_STARTED on first thinking content\n        if (!hasEmittedStepStarted) {\n          hasEmittedStepStarted = true\n          stepId = generateId('step')\n          yield {\n            type: 'STEP_STARTED',\n            stepId,\n            model: chunk.model,\n            timestamp,\n            stepType: 'thinking',\n          }\n        }\n\n        accumulatedReasoning += chunk.message.thinking\n        yield {\n          type: 'STEP_FINISHED',\n          stepId: stepId || generateId('step'),\n          model: chunk.model,\n          timestamp,\n          delta: chunk.message.thinking,\n          content: accumulatedReasoning,\n        }\n      }\n    }\n  }\n\n  private convertToolsToOllamaFormat(\n    tools?: Array<Tool>,\n  ): Array<OllamaTool> | undefined {\n    if (!tools || tools.length === 0) {\n      return undefined\n    }\n\n    // Tool schemas are already converted to JSON Schema in the ai layer.\n    // We use a type assertion because our JSONSchema type is more flexible\n    // than ollama's expected schema type (e.g., type can be string | string[]).\n    return tools.map((tool) => ({\n      type: 'function',\n      function: {\n        name: tool.name,\n        description: tool.description,\n        parameters: (tool.inputSchema ?? {\n          type: 'object',\n          properties: {},\n          required: [],\n        }) as OllamaTool['function']['parameters'],\n      },\n    }))\n  }\n\n  private formatMessages(messages: TextOptions['messages']): Array<Message> {\n    return messages.map((msg) => {\n      let textContent = ''\n      const images: Array<string> = []\n\n      if (Array.isArray(msg.content)) {\n        for (const part of msg.content) {\n          if (part.type === 'text') {\n            textContent += part.content\n          } else if (part.type === 'image') {\n            if (part.source.type === 'data') {\n              images.push(part.source.value)\n            } else {\n              images.push(part.source.value)\n            }\n          }\n        }\n      } else {\n        textContent = msg.content || ''\n      }\n\n      const hasToolCallId = msg.role === 'tool' && msg.toolCallId\n      return {\n        role: hasToolCallId ? 'tool' : msg.role,\n        content: hasToolCallId\n          ? typeof msg.content === 'string'\n            ? msg.content\n            : JSON.stringify(msg.content)\n          : textContent,\n        ...(images.length > 0 ? { images } : {}),\n        ...(msg.role === 'assistant' &&\n        msg.toolCalls &&\n        msg.toolCalls.length > 0\n          ? {\n              tool_calls: msg.toolCalls.map((toolCall) => {\n                let parsedArguments: Record<string, unknown> = {}\n                if (typeof toolCall.function.arguments === 'string') {\n                  try {\n                    parsedArguments = JSON.parse(\n                      toolCall.function.arguments,\n                    ) as Record<string, unknown>\n                  } catch {\n                    parsedArguments = {}\n                  }\n                } else {\n                  parsedArguments = toolCall.function\n                    .arguments as unknown as Record<string, unknown>\n                }\n\n                return {\n                  id: toolCall.id,\n                  type: toolCall.type,\n                  function: {\n                    name: toolCall.function.name,\n                    arguments: parsedArguments,\n                  },\n                }\n              }),\n            }\n          : {}),\n      }\n    })\n  }\n\n  private mapCommonOptionsToOllama(options: TextOptions): ChatRequest {\n    const model = options.model\n    const modelOptions = options.modelOptions as\n      | OllamaTextProviderOptions\n      | undefined\n\n    const ollamaOptions = {\n      temperature: options.temperature,\n      top_p: options.topP,\n      num_predict: options.maxTokens,\n      ...modelOptions,\n    }\n\n    return {\n      model,\n      options: ollamaOptions,\n      messages: this.formatMessages(options.messages),\n      tools: this.convertToolsToOllamaFormat(options.tools),\n    }\n  }\n}\n\n/**\n * Creates an Ollama chat adapter with explicit host.\n * Type resolution happens here at the call site.\n */\nexport function createOllamaChat<TModel extends string>(\n  model: TModel,\n  host?: string,\n): OllamaTextAdapter<TModel> {\n  return new OllamaTextAdapter(host, model)\n}\n\n/**\n * Creates an Ollama text adapter with host from environment.\n * Type resolution happens here at the call site.\n */\nexport function ollamaText<TModel extends string>(\n  model: TModel,\n): OllamaTextAdapter<TModel> {\n  const host = getOllamaHostFromEnv()\n  return new OllamaTextAdapter(host, model)\n}\n"],"names":[],"mappings":";;AA8GO,MAAM,0BAAiD,gBAK5D;AAAA,EAMA,YAAY,cAA2C,OAAe;AACpE,UAAM,CAAA,GAAI,KAAK;AANjB,SAAS,OAAO;AAChB,SAAS,OAAO;AAMd,QAAI,OAAO,iBAAiB,YAAY,iBAAiB,QAAW;AAClE,WAAK,SAAS,mBAAmB,EAAE,MAAM,cAAc;AAAA,IACzD,OAAO;AACL,WAAK,SAAS;AAAA,IAChB;AAAA,EACF;AAAA,EAEA,OAAO,WAAW,SAAkD;AAClE,UAAM,gBAAgB,KAAK,yBAAyB,OAAO;AAC3D,UAAM,WAAW,MAAM,KAAK,OAAO,KAAK;AAAA,MACtC,GAAG;AAAA,MACH,QAAQ;AAAA,IAAA,CACT;AACD,WAAO,KAAK,0BAA0B,QAAQ;AAAA,EAChD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,EAOA,MAAM,iBACJ,SAC0C;AAC1C,UAAM,EAAE,aAAa,aAAA,IAAiB;AAEtC,UAAM,gBAAgB,KAAK,yBAAyB,WAAW;AAE/D,QAAI;AAEF,YAAM,WAAW,MAAM,KAAK,OAAO,KAAK;AAAA,QACtC,GAAG;AAAA,QACH,QAAQ;AAAA,QACR,QAAQ;AAAA,MAAA,CACT;AAED,YAAM,UAAU,SAAS,QAAQ;AAGjC,UAAI;AACJ,UAAI;AACF,iBAAS,KAAK,MAAM,OAAO;AAAA,MAC7B,QAAQ;AACN,cAAM,IAAI;AAAA,UACR,uDAAuD,QAAQ,MAAM,GAAG,GAAG,CAAC,GAAG,QAAQ,SAAS,MAAM,QAAQ,EAAE;AAAA,QAAA;AAAA,MAEpH;AAEA,aAAO;AAAA,QACL,MAAM;AAAA,QACN;AAAA,MAAA;AAAA,IAEJ,SAAS,OAAgB;AACvB,YAAM,MAAM;AACZ,YAAM,IAAI;AAAA,QACR,wCAAwC,IAAI,WAAW,wBAAwB;AAAA,MAAA;AAAA,IAEnF;AAAA,EACF;AAAA,EAEA,OAAe,0BACb,QAC4B;AAC5B,QAAI,qBAAqB;AACzB,UAAM,YAAY,KAAK,IAAA;AACvB,QAAI,uBAAuB;AAC3B,UAAM,uCAAuB,IAAA;AAG7B,UAAM,QAAQ,WAAW,KAAK;AAC9B,UAAM,YAAY,WAAW,KAAK;AAClC,QAAI,SAAwB;AAC5B,QAAI,uBAAuB;AAC3B,QAAI,6BAA6B;AACjC,QAAI,wBAAwB;AAE5B,qBAAiB,SAAS,QAAQ;AAEhC,UAAI,CAAC,sBAAsB;AACzB,+BAAuB;AACvB,cAAM;AAAA,UACJ,MAAM;AAAA,UACN;AAAA,UACA,OAAO,MAAM;AAAA,UACb;AAAA,QAAA;AAAA,MAEJ;AAEA,YAAM,iBAAiB,CAAC,aAA2C;AACjE,cAAM,iBAAiB;AAIvB,cAAM,aACJ,eAAe,MAAM,GAAG,eAAe,SAAS,IAAI,IAAI,KAAK,IAAA,CAAK;AACpE,cAAM,SAA6B,CAAA;AAGnC,YAAI,CAAC,iBAAiB,IAAI,UAAU,GAAG;AACrC,2BAAiB,IAAI,UAAU;AAC/B,iBAAO,KAAK;AAAA,YACV,MAAM;AAAA,YACN;AAAA,YACA,UAAU,eAAe,SAAS,QAAQ;AAAA,YAC1C,OAAO,MAAM;AAAA,YACb;AAAA,YACA,OAAO,eAAe,SAAS;AAAA,UAAA,CAChC;AAAA,QACH;AAGA,YAAI,cAAuB,CAAA;AAC3B,cAAM,UACJ,OAAO,eAAe,SAAS,cAAc,WACzC,eAAe,SAAS,YACxB,KAAK,UAAU,eAAe,SAAS,SAAS;AACtD,YAAI;AACF,wBAAc,KAAK,MAAM,OAAO;AAAA,QAClC,QAAQ;AACN,wBAAc,eAAe,SAAS;AAAA,QACxC;AAGA,eAAO,KAAK;AAAA,UACV,MAAM;AAAA,UACN;AAAA,UACA,UAAU,eAAe,SAAS,QAAQ;AAAA,UAC1C,OAAO,MAAM;AAAA,UACb;AAAA,UACA,OAAO;AAAA,QAAA,CACR;AAED,eAAO;AAAA,MACT;AAEA,UAAI,MAAM,MAAM;AACd,YAAI,MAAM,QAAQ,cAAc,MAAM,QAAQ,WAAW,SAAS,GAAG;AACnE,qBAAW,YAAY,MAAM,QAAQ,YAAY;AAC/C,kBAAM,SAAS,eAAe,QAAQ;AACtC,uBAAW,SAAS,QAAQ;AAC1B,oBAAM;AAAA,YACR;AAAA,UACF;AAAA,QACF;AAGA,YAAI,4BAA4B;AAC9B,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN;AAAA,YACA,OAAO,MAAM;AAAA,YACb;AAAA,UAAA;AAAA,QAEJ;AAEA,cAAM;AAAA,UACJ,MAAM;AAAA,UACN;AAAA,UACA,OAAO,MAAM;AAAA,UACb;AAAA,UACA,cAAc,iBAAiB,OAAO,IAAI,eAAe;AAAA,QAAA;AAE3D;AAAA,MACF;AAEA,UAAI,MAAM,QAAQ,SAAS;AAEzB,YAAI,CAAC,4BAA4B;AAC/B,uCAA6B;AAC7B,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN;AAAA,YACA,OAAO,MAAM;AAAA,YACb;AAAA,YACA,MAAM;AAAA,UAAA;AAAA,QAEV;AAEA,8BAAsB,MAAM,QAAQ;AACpC,cAAM;AAAA,UACJ,MAAM;AAAA,UACN;AAAA,UACA,OAAO,MAAM;AAAA,UACb;AAAA,UACA,OAAO,MAAM,QAAQ;AAAA,UACrB,SAAS;AAAA,QAAA;AAAA,MAEb;AAEA,UAAI,MAAM,QAAQ,cAAc,MAAM,QAAQ,WAAW,SAAS,GAAG;AACnE,mBAAW,YAAY,MAAM,QAAQ,YAAY;AAC/C,gBAAM,SAAS,eAAe,QAAQ;AACtC,qBAAW,SAAS,QAAQ;AAC1B,kBAAM;AAAA,UACR;AAAA,QACF;AAAA,MACF;AAEA,UAAI,MAAM,QAAQ,UAAU;AAE1B,YAAI,CAAC,uBAAuB;AAC1B,kCAAwB;AACxB,mBAAS,WAAW,MAAM;AAC1B,gBAAM;AAAA,YACJ,MAAM;AAAA,YACN;AAAA,YACA,OAAO,MAAM;AAAA,YACb;AAAA,YACA,UAAU;AAAA,UAAA;AAAA,QAEd;AAEA,gCAAwB,MAAM,QAAQ;AACtC,cAAM;AAAA,UACJ,MAAM;AAAA,UACN,QAAQ,UAAU,WAAW,MAAM;AAAA,UACnC,OAAO,MAAM;AAAA,UACb;AAAA,UACA,OAAO,MAAM,QAAQ;AAAA,UACrB,SAAS;AAAA,QAAA;AAAA,MAEb;AAAA,IACF;AAAA,EACF;AAAA,EAEQ,2BACN,OAC+B;AAC/B,QAAI,CAAC,SAAS,MAAM,WAAW,GAAG;AAChC,aAAO;AAAA,IACT;AAKA,WAAO,MAAM,IAAI,CAAC,UAAU;AAAA,MAC1B,MAAM;AAAA,MACN,UAAU;AAAA,QACR,MAAM,KAAK;AAAA,QACX,aAAa,KAAK;AAAA,QAClB,YAAa,KAAK,eAAe;AAAA,UAC/B,MAAM;AAAA,UACN,YAAY,CAAA;AAAA,UACZ,UAAU,CAAA;AAAA,QAAC;AAAA,MACb;AAAA,IACF,EACA;AAAA,EACJ;AAAA,EAEQ,eAAe,UAAmD;AACxE,WAAO,SAAS,IAAI,CAAC,QAAQ;AAC3B,UAAI,cAAc;AAClB,YAAM,SAAwB,CAAA;AAE9B,UAAI,MAAM,QAAQ,IAAI,OAAO,GAAG;AAC9B,mBAAW,QAAQ,IAAI,SAAS;AAC9B,cAAI,KAAK,SAAS,QAAQ;AACxB,2BAAe,KAAK;AAAA,UACtB,WAAW,KAAK,SAAS,SAAS;AAChC,gBAAI,KAAK,OAAO,SAAS,QAAQ;AAC/B,qBAAO,KAAK,KAAK,OAAO,KAAK;AAAA,YAC/B,OAAO;AACL,qBAAO,KAAK,KAAK,OAAO,KAAK;AAAA,YAC/B;AAAA,UACF;AAAA,QACF;AAAA,MACF,OAAO;AACL,sBAAc,IAAI,WAAW;AAAA,MAC/B;AAEA,YAAM,gBAAgB,IAAI,SAAS,UAAU,IAAI;AACjD,aAAO;AAAA,QACL,MAAM,gBAAgB,SAAS,IAAI;AAAA,QACnC,SAAS,gBACL,OAAO,IAAI,YAAY,WACrB,IAAI,UACJ,KAAK,UAAU,IAAI,OAAO,IAC5B;AAAA,QACJ,GAAI,OAAO,SAAS,IAAI,EAAE,OAAA,IAAW,CAAA;AAAA,QACrC,GAAI,IAAI,SAAS,eACjB,IAAI,aACJ,IAAI,UAAU,SAAS,IACnB;AAAA,UACE,YAAY,IAAI,UAAU,IAAI,CAAC,aAAa;AAC1C,gBAAI,kBAA2C,CAAA;AAC/C,gBAAI,OAAO,SAAS,SAAS,cAAc,UAAU;AACnD,kBAAI;AACF,kCAAkB,KAAK;AAAA,kBACrB,SAAS,SAAS;AAAA,gBAAA;AAAA,cAEtB,QAAQ;AACN,kCAAkB,CAAA;AAAA,cACpB;AAAA,YACF,OAAO;AACL,gCAAkB,SAAS,SACxB;AAAA,YACL;AAEA,mBAAO;AAAA,cACL,IAAI,SAAS;AAAA,cACb,MAAM,SAAS;AAAA,cACf,UAAU;AAAA,gBACR,MAAM,SAAS,SAAS;AAAA,gBACxB,WAAW;AAAA,cAAA;AAAA,YACb;AAAA,UAEJ,CAAC;AAAA,QAAA,IAEH,CAAA;AAAA,MAAC;AAAA,IAET,CAAC;AAAA,EACH;AAAA,EAEQ,yBAAyB,SAAmC;AAClE,UAAM,QAAQ,QAAQ;AACtB,UAAM,eAAe,QAAQ;AAI7B,UAAM,gBAAgB;AAAA,MACpB,aAAa,QAAQ;AAAA,MACrB,OAAO,QAAQ;AAAA,MACf,aAAa,QAAQ;AAAA,MACrB,GAAG;AAAA,IAAA;AAGL,WAAO;AAAA,MACL;AAAA,MACA,SAAS;AAAA,MACT,UAAU,KAAK,eAAe,QAAQ,QAAQ;AAAA,MAC9C,OAAO,KAAK,2BAA2B,QAAQ,KAAK;AAAA,IAAA;AAAA,EAExD;AACF;AAMO,SAAS,iBACd,OACA,MAC2B;AAC3B,SAAO,IAAI,kBAAkB,MAAM,KAAK;AAC1C;AAMO,SAAS,WACd,OAC2B;AAC3B,QAAM,OAAO,qBAAA;AACb,SAAO,IAAI,kBAAkB,MAAM,KAAK;AAC1C;"}